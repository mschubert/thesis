#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Gene expression methods for cancer cell line drug sensitivity and patient
 outcome
\end_layout

\begin_layout Part
Introduction
\end_layout

\begin_layout Section
Cancer Biology
\end_layout

\begin_layout Subsection
Significance and Etiology
\end_layout

\begin_layout Itemize
number of patients affected
\end_layout

\begin_layout Itemize
survival changes in the last 40 years
\end_layout

\begin_layout Itemize
disease of the genes
\end_layout

\begin_layout Itemize
early detection is key
\end_layout

\begin_layout Subsection
Mutational Processes
\end_layout

\begin_layout Itemize
SNVs vs structural rearrangements
\end_layout

\begin_layout Itemize
ludmil's paper
\end_layout

\begin_layout Itemize
different tissues: different forces
\end_layout

\begin_layout Subsubsection
Germline variation
\end_layout

\begin_layout Standard
In the human population, there is natural variation in the genome from one
 individual to another, which gets passed down through generations in the
 process of reproduction.
 Each human being has got two copies of their DNA, one passed down from
 their mother and the other passed down from their father.
 Disregarding of where the copies that an individal actually inherits came
 from, they may harbor the same sequence (that is, the individual is homozygous
 in a given base or gene) or may be different between those (that is, the
 individual is heterozygous).
 This is (in part) what makes us look different, but also what may cause
 us to be more or less susceptible to a certain disease or treatment thereof.
 Since the genes were assembled in the fertilized egg already that afterwards
 underwent cell divisions (in a process also called mitosis), all the cells
 in our body in theory harbour the same DNA sequence, with the exception
 that sex cells only contain one copy that was assembled in a cut-and-paste
 process (called meiosis) of the two somatic ones.
\end_layout

\begin_layout Subsubsection
Somatic variation
\end_layout

\begin_layout Standard
Should we sequence each cell in an individual, we would not find that all
 of their consecutive bases made up of As, Ts, Cs, and Gs are indeed identical,
 but there also variation within each individual.
 There may be a change that is common to cells that derive from the same
 precursor, or a change may have been introduced in a single cell by an
 internal or external process.
 In the former case, we need to realize that the DNA copy mechanism (also
 called DNA replication) that ensures that that wenn a cell splits both
 its daughter cells inherit two full copies of its DNA is not infinitely
 accurate, but this process may introduce reading- (from the ancestral or
 template strand) and writing (the newly synthesised strand) errors.
 In fact, in each cell division there is an average of *** errors that are
 introduced while copying the each of the three billion bases (times the
 two alleles) that is our genome.
\end_layout

\begin_layout Standard
Such an error may manifest itself in multiple ways.
 The replication apparatus (DNA polymerase) may miss the insertion of a
 base or a couple of bases that was in the ancestral in the newly synthesised
 strand, which produces a small deletion.
 In contrast, it also may insert a base that was not in the original strand,
 thereby producing an insertion.
 It may also insert the wrong base, which leads to a substitution.
 Even when the replication process produced a perfect copy of the ancestral
 strands, there may still be cell-internal or external processes that affect
 the DNA's integrity - called mutations.
\end_layout

\begin_layout Standard
While discussing theses mutational processes one should keep in mind that
 those errors introduced do manifest itself only on one of the two strands
 in an allele, which will lead to a base not perfectly matching its opposite
 base anymore, which means they do not pair as well as they were if there
 had been no error.
\end_layout

\begin_layout Subsubsection
Mutations
\end_layout

\begin_layout Standard
...
\end_layout

\begin_layout Subsection
Consequences
\end_layout

\begin_layout Itemize
Oncogenes and Tumor Suppressors
\end_layout

\begin_layout Itemize
not all genes equally important
\end_layout

\begin_layout Itemize
mutations switch on+off genes
\end_layout

\begin_layout Itemize
cell signaling
\end_layout

\begin_layout Standard
Genes that are relevant for cancer development and progression have long
 been classified in Oncogenes and Tumor Suppressor Genes.
 In broad terms, this classification represents the following: does a mutation
 introduce a gene function that was not there before, or it loses sensitivty
 to an inhibitory signal that kept it in check, this gene is called an Oncogene
 in its active (mutated) form, or Proto-Oncogene in its wild-type form;
 conversely, if a mutation causes a gene a lose its function as e.g.
 a negative regulator of cell growth, it is called an Tumour Suppressor
 Gene.
 The first Tumour Suppressor Gene that has been found was TP53 in ***, now
 termed 
\begin_inset Quotes eld
\end_inset

Guardian of the Cell Cycle
\begin_inset Quotes erd
\end_inset

 for its numerous functions maintaining DNA integrity, managing DNA repair,
 and causing apoptosis or senescence of the former fail.
 More recently, mutations that actively contribute to either development
 of progression of a cancer have been called drivers, while the mutations
 introduced by e.g.
 a faulty DNA replication machinery (that a cancer may have caused) that
 bear no functional impact on the cell are called passenger mutations.
 Driver genes are usually either activated Oncogenes or Tumour suppressor
 genes, but may also have both functions.
\end_layout

\begin_layout Standard
A gene that has an erroneous sequence somewhere on the DNA is, by itself,
 not a cause for a cell to alter its function.
 Instead, genes only store the information, like having a template, that
 is required to form an active compound from it.
 This may be a regulatory RNA, or a messenger RNA (mRNA; both derived from
 DNA by a process called transcription) that is later used by the Ribosome
 to chain amino acids together to a functional protein in a process called
 translation.
 The path from mRNA to protein may involve additional steps, such as cutting
 out unneeded parts or pasting together different building blocks (splicing),
 or covalently attaching sugars or other chemical groups (post-translational
 modifications) to yield the functional protein and/or to regulate its activity.
\end_layout

\begin_layout Standard
Take, for instance, a protein is involed in relaying a signal from the cell
 surface, such as a signal to grow and divide, to another protein that in
 turn activates some other proteins which ultimately induces the expression
 of genes that are needed for the cell's replication machinery.
 These signals are usually relayed by post-translational modifications from
 one protein to another.
 One of the best-studied processes is that of phosphorylation (the addition
 of a phosphate group - proteins that do that are called kinases, the ones
 that remove phosphatases) on Serines or Threonines (that have an OH group
 in their side chain and are thus susceptible to it).
\end_layout

\begin_layout Standard
This protein now may be affected by a mutation so that it no longer waits
 for an upstream signal to transduce, but sends the downstream signal to
 grow and divide irrespective of what the upstream input is.
 One such example is the well-known BRAF_V600E mutation, where a Valine
 on position 600 (the 600th amino acid) is replaced by Glutamic Acid.
\end_layout

\begin_layout Subsection
Cellular Coping Mechanisms
\end_layout

\begin_layout Itemize
peter campbell's non-cancerous skin high depth -> Q should be: not how it
 develops, but how does it not?
\end_layout

\begin_layout Itemize
DNA repair
\end_layout

\begin_layout Itemize
senescence
\end_layout

\begin_layout Itemize
apoptosis
\end_layout

\begin_layout Itemize
immune clearance
\end_layout

\begin_layout Standard
There are however, cellular mechanisms that identify and try to fix those
 issues.
 The only problem is: how will they recognise which is the correct and which
 is the faulty strand?
\end_layout

\begin_layout Subsection
Uncontrolled Growth
\end_layout

\begin_layout Itemize
go through all hanahan & weinberg Hallmarks
\end_layout

\begin_layout Section
Therapeutic Interventions
\end_layout

\begin_layout Subsection
The Therapeutic Window
\end_layout

\begin_layout Itemize
bioavailability
\end_layout

\begin_layout Itemize
difference in killing normal vs target cells
\end_layout

\begin_layout Standard
Within a therapeutic intervention, our goal is to selectively treat (or
 kill) the diseased cells while not impacting the normal function of other
 cells in the body.
 If at first we assume that a treatment can be delivered to all cells of
 the body in the same amount, the question is whether the impact it has
 is impacting the cells we want to act on more than all the other cells
 we want to impact as little as possible.
 If we take the simple example of killing cells, we want a drug that kills
 all the bad cells and leaves the good cells alone.
 An effective drug will work on the bad cells at a lower concentration than
 on the good ones.
 If the concentration is too high, it will undoubtely affect other cells
 as well.
 This range of concentration, where a given drug already acts on the target
 cells but does not confer toxicity to other cells is called the therapeutic
 window.
 The broader it is, the easier it is to work with this drug.
\end_layout

\begin_layout Standard
Elaborating on our simplification, it is of course not the case that an
 administered compound will be available to all cells at the same concentration.
 Instead, it first needs to reach the bloodstream (which includes uptake
 and resorption, then modification in the liver [*?] and pancreas if taken
 orally), then be distributed by the flow of blood into all the capillaries
 (which the exception of the brain that has an additional barrier), until
 finally cells are able to absorb it.
 As a rule of thumb whether a drug can be orally absorbed or not, a common
 measure is 
\begin_inset Quotes eld
\end_inset

Lipinsky's Rule of Five
\begin_inset Quotes erd
\end_inset

, that states the following: 
\end_layout

\begin_layout Subsection
Cytotoxic Drugs
\end_layout

\begin_layout Itemize
based on cells growing quicker/consuming more resources
\end_layout

\begin_layout Standard
...put some examples of clinical cyototoxic here...
\end_layout

\begin_layout Subsection
Targeted Therapies
\end_layout

\begin_layout Itemize
specific blockade of signaling pathway
\end_layout

\begin_layout Itemize
small molecules vs antibodies
\end_layout

\begin_layout Itemize
put some intro to chem bio in here?
\end_layout

\begin_layout Standard
A recurrent theme of cancer development and progression is the aberrant
 activation of specific molecular cues in cell signaling.
 An example of this is the well-known BRAF-V600E mutation, already mentioned
 in section [consequences].
 But there are many more instances where a mutation in a gene yields a gene
 product that is abnormally active- or inactive.
 This often includes members of the MAP kinase pathway (like EGFR, RAS,
 RAF, MEK, or ERK), but frequently - and often strongly correlated with
 the general cell state as is defined by, for instance, the tissue of origin
 - other proteins and pathways that confer a selective advantage in a given
 context.
 An important aspect in that regard is the concept of oncogene addiction:
 Once a cell, or a population of cells, suffers a molecular lesion that
 causes aberrant signaling, the cell (or cells) become dependent on exactly
 that signal.
 Turn it off, and those cells are likely to die.
\end_layout

\begin_layout Standard
This has an important implication for cancer therapy: if we are able to
 find (or design) a compound that specifically turns off one of those abnormally
 active proteins, we can kill cells that have it active.
 
\end_layout

\begin_layout Standard
Coming back to our example of the BRAF-V600E mutation, [* et al] indeed
 designed and developed a small-molecule inhibitor called Plexicon (more
 specifically, PLX4720) that proved to bind the mutation version of the
 BRAF protein with a much higher affinity than the one found in unmutated
 cells.
 This, in some ways, could be considered the perfect drug for cancer therapy,
 because it has it has favourable uptake in the body [*], and once it is
 there the therapeutic window is much larger compared to other compounds
 due to this difference in binding affinity.
\end_layout

\begin_layout Standard
...put some examples of clinical targeted therapies here...
\end_layout

\begin_layout Subsection
Immune Therapy
\end_layout

\begin_layout Itemize
PD-1, PD-L1 inhibitors
\end_layout

\begin_layout Standard
When the President of the United States announced the so-called Moonshot
 effort that promised funding for developing and improving personalised
 cancer treatment by targeted therapies, immune therapies were set to play
 a profound role in it.
\end_layout

\begin_layout Subsection
Development of Resistance
\end_layout

\begin_layout Itemize
kills most cells, but some survive and repopulate
\end_layout

\begin_layout Itemize
cancer stem cells
\end_layout

\begin_layout Standard
Let us revisit one of the most well-known examples in targeted therapies,
 the inhibitor Plexicon (PLX4720) specifically targeted to BRAF-V600E mutants.
 In a seminal paper, *** et al.
 showed that this example of targeted therapy works with the intended effect
 in such that it kills cancer cells to an extent that makes multiple from
 the outside clearly visible tumours completely disappear for months, which
 could be hailed as a success for identifying a target and rationally designing
 an inhibitor that abrogates oncogenic signaling.
 However, there was one drawback: after a couple more weeks after the initial
 success, the tumour cells were able to overcome the effects of the inhibitor,
 start to grow again, and ultimately lead to the recurrence of the tumours
 that the patient later died from.
\end_layout

\begin_layout Standard
This, together with multiple other examples, proved to show that targeted
 therapies work for a while, but they are, in a lot of cases, unable to
 prevent recurrence.
 But how does this work? In order for a tumour to regrow, there need to
 be some cancerous cells left alive after treatment, as this rapid regrowth
 of mass can not be explained by an idependent inception, so some cells
 must have survived the process of therapy.
 More specifically, there are two possibilities of these cells that later
 repopulate a tumour can be formed (and a spectrum in between): (1) the
 applied selection pressure (the therapy killing off sensitive cells) caused,
 together with a rapid accumulation new mutations, different pools or cells
 to emerge where a subset was no longer affected by the drug and that could
 later expand, or (2) those resistant cells were there all along, just expanding
 their niche once the predominant clones were gone.
\end_layout

\begin_layout Standard
This debate lasted for a while, evidence presented for both one [*] and
 the other [*]: surely, if there are cells present that harbor a mutation
 that confers resistance to a given drug at the beginning of treatment,
 sequencing the tumour must show a minor frequency of this mutation that
 later takes over already at that stage.
 However, there are a couple of challenges when trying to assess the whole
 genetic diversity of a sample.
 For instance, tumours are often histologically quite different already
 across different section, implying that the molecular makeup is not uniformly
 distributed.
 Thus, it is important to sample not only in one place, but either mush
 the whole thing up or selectively take small samples in different subsections.
 Another challenge is that for detecting very minor frequencies, one needs
 to sequence at a high depth and be sure that the resulting variants are
 not only due to sequencing errors (if there is little enough DNA that it
 needs to be amplified, errors introduced by PCR need also be considered).
 It eventually settled when some groups [*] were able to overcome those
 challenges and show that the resistant clone is indeed present at low frequency
 at the beginning of therapy.
\end_layout

\begin_layout Section
Disease models
\end_layout

\begin_layout Subsection
Rationale
\end_layout

\begin_layout Itemize
data from treatment directly
\end_layout

\begin_layout Itemize
can't try out stuff
\end_layout

\begin_layout Itemize
anything that is not best known treatment unethical
\end_layout

\begin_layout Itemize
mirror the disease inception, progression, and treatment
\end_layout

\begin_layout Itemize
clinical trials
\end_layout

\begin_layout Standard
It is easy to argue that in order to find better cancer treatments, we first
 need to understand better the molecular mechanisms that drive it.
 This is especially true because the mechanisms involved from the inception
 of cancer up the its progression and spreading are in fact molecular mechanisms
 that have spun out of the normal biological control that cells of an organism
 excert on each other.
 In order to improve our understanding, it is required to collect data about
 the different types, stages, treatments, and similar.
\end_layout

\begin_layout Standard
This poses a problem: it is not possible to perform all of the assays required
 on the actual patients.
 There is just no justification for a patient to undergo surgery if we want
 to know if protein A interacts with protein B.
 Also with treatments, we can not try new treatments not having a strong
 indication that this might be the best known possibility of curing a patient,
 as this would be highly unethical.
 It follows that there is a requirement for some biological system that
 mirrors the disease while, at the same time, is easy to handle in the laborator
y.
 Therein, the points mentioned involve a tradeoff: a system that mirrors
 the disease perfectly and is easy to handle does not exist, but there are
 multiple systems (outlined in the sections below) that are closer to one
 or the other.
 Which one to use must be decided in each experiment individually - considering
 its goals, effort, conclusiveness and applicability to the question studied.
\end_layout

\begin_layout Standard
However, if there is a strong indication given those model systems that
 a certain treatment approach is truly beneficial to a certain patient or
 patient group, this needs to be thoroughly tested to make sure that those
 indications previously shown using a model system - that are known to mirror
 a lot of aspects of the actual disease, but never all - actually hold in
 humans as well.
\end_layout

\begin_layout Subsection
Cell Lines
\end_layout

\begin_layout Itemize
once established, easy to grow
\end_layout

\begin_layout Itemize
unlimited supply
\end_layout

\begin_layout Itemize
easy handling (plastic dishes)
\end_layout

\begin_layout Itemize
compared to others: uniform
\end_layout

\begin_layout Standard
One of the easiest model systems that provide a reasonable accurate mirror
 of a lot of the biology involved in the disease are cell lines, which is
 basically taking a bunch of cells from a tumour, and growing them in a
 petri dish for multiple generations (passages).
 An obvious advantage of this method is that the biological material that
 assays can be performed on is virtually unlimited, easy to produce and
 maintain (the cells grow in the dishes as long as they are supplied with
 nutrients and potentially growth factors or cytokines), while still reflecting
 a lot of the properties that cells in a tumour would.
 However, by passaging cells consecutively in petri dishes for many generations,
 the evolutionary pressure that they are selected by is essentially the
 growth rate on the dish, which will not reflect the proportions that one
 would find in a real tumour.
 In fact, the fastest growing clone will outcompete all others in a couple
 of generations - which gives a relatively uniform molecular phenotype across
 all cells, but in turn also loses the heterogeneity found in a primary
 sample.
\end_layout

\begin_layout Standard
Today, there is a multitude of cell lines available from commercial suppliers.
 The maybe best-known example is that of the HeLa cell line that was derived
 from a patient of cervical cancer in 1973, named Henrietta Lacks.
 This cell line, however, with how long it has been kept in culture as well
 as its high mutation rate have produced a genotype that no longer resembles
 primary cancer samples in many ways.
 One property is that a normal human cell has two copies of the genome (the
 two alleles), whereas HeLa cells have four copies.
\end_layout

\begin_layout Standard
In contrast to this cell line, most on which experiments are performed on
 more closely resemble their origin: they have, albeit with aberrations,
 a genome that somewhat resembles primary tumours; they also in many cases
 still resemble their tissue of origin in terms of gene expression patterns
 and transcription factor activities.
 However, there are biological dynamics involved in a real tumour that can
 not be recapitulated by cell lines.
 These include interactions with other cell types (where a co-culture is
 possible, but not anywhere close in complexity), interactions with the
 extracellular matrix (remember the flat plastic dishes they are grown in).
 Another one, which can be seen as a special case of the former, is the
 lacking interaction with immune cells.
\end_layout

\begin_layout Subsection
Organoids
\end_layout

\begin_layout Itemize
some signaling pathways that are important in patients not captured in cell
 lines, but potentially here
\end_layout

\begin_layout Itemize
generating much more diversity than there can be cell lines
\end_layout

\begin_layout Subsection
Animal models
\end_layout

\begin_layout Itemize
KO/KI: making more prone
\end_layout

\begin_layout Itemize
xenografts
\end_layout

\begin_layout Itemize
similar but not the same to human
\end_layout

\begin_layout Standard
Of course, there are aspects missing when relying on models that handle
 individual cells or the interaction of a couple of cell of the same type
 when trying to assess either mechanisms of pathogenesis or in vivo response
 to a compound, which makes it necessary to perform experiments in a whole
 organism.
 For this purpose animals have long been used, down from a simple multicellular
 worm (Caenorhabdilis elegans, mostly for development and its simple central
 nervous system) to frogs (Xenopus, for induced pluripotency), up to mammals
 (mice, rats, and chimps).
 Just as the complexity of these organisms increases, they also mirror more
 and more aspects of human physiology.
 However, experiments performed in the latter organisms also takes much
 more time, they are more difficult (and expensive) to set up, and there
 are ethical concerns on keeping, handling, and killing animals for the
 purpose of ultimately saving human lives.
 Still, none of the animals mentioned is human, so there the unknown factors
 as to whether a certain drug that has been shown to work in cell lines
 and mice does indeed have the same effect in humans remains.
 Looking at the number of articles published about a specific molecular
 mechanism or the detailed characterisation of a compound and its usability
 as a drug, mice seem to be considered a reasonable tradeoff between providing
 a close enough match of human biology and being not too difficult to keep.
\end_layout

\begin_layout Standard
One line of experiments that can be done with mice but not humans is to
 edit genes in their germline, and then breed them with a reporter or until
 their offspring is homozygous for a desired mutation.
 The gene introduced can either be a reporter, to see where a it is expressed,
 or for instance a mutated version of a human gene that is known to induce
 the formation of tumors.
\end_layout

\begin_layout Standard
Another method of invastigation are xenografts, where an immunodeficient
 [*?*] mouse is inoculated with cancerous cells derived from either a cell
 line or a patient, to study effects like how cancerous cells influence
 and modify the microenvironment around them, how they can sustain themselves
 and grow, attract the formation of blood vessels, and ultimately metastisise.
\end_layout

\begin_layout Subsection
Models used in this work
\end_layout

\begin_layout Itemize
drug screening
\end_layout

\begin_layout Itemize
perturbed expr
\end_layout

\begin_layout Itemize
patient survival
\end_layout

\begin_layout Standard
All drug screening experiments and analysis based thereon in this work,
 while there are potentially many drawbacks associated with it, have been
 carried out on data that was derived from cell lines.
 This can easily explained by the fact that the amount that needed to be
 generated in order to make general statements about perturbed gene expression
 and especially drug response is not feasable with any other model system.
\end_layout

\begin_layout Standard
For cancer-specific gene expression, I used either used the same cell lines,
 or patient data directly from a consortium whose aim it is to molecularly
 characterise a multitude of cancers (The Cancer Genome Atlas consortium,
 TCGA).
 It is also the latter where I obtained survival data from, as more thorougly
 described in section *.
\end_layout

\begin_layout Section
Molecular Data Types and Assays
\end_layout

\begin_layout Subsection
Role of Molecular Data
\end_layout

\begin_layout Itemize
this is the data we get from the models
\end_layout

\begin_layout Standard
One of the issues of both understanding as well as preventing or treating
 cancer development and progression is that it is a process in such complexity
 that a simple observation of patients with the disease will not suffice
 to come up with effective models of disease inception and progression,
 or treatmants for that matter.
 Fortunately, we have got a battery of tests available, quantifying different
 molecular aspects of a biological sample.
 Examples of these data include the sequence, structure, and modification
 of DNA, but also the expression of RNA or proteins, including modifications.
 These different layers, able to quantify different projections of a cell's
 state, have provided us with an unprecedented opportunity to truly understand
 many molecular mechanisms that govern the processes running in both a normal
 and a diseased cell, and the differences between them.
\end_layout

\begin_layout Subsection
DNA
\end_layout

\begin_layout Itemize
sanger seq of few genes
\end_layout

\begin_layout Itemize
NGS
\end_layout

\begin_layout Itemize
methylation
\end_layout

\begin_layout Subsection
RNA
\end_layout

\begin_layout Itemize
microarrays: single vs dual channel
\end_layout

\begin_layout Itemize
RNA-seq: unbiased, alignment to genome vs assembly, alignment-free methods
\end_layout

\begin_layout Standard
In contrast to DNA, which is stable in sequence, and its chromatin structure
 and methylation that are still, albeit a bit less stable, the RNA that
 is transcribed in order to perform some regulatory functions as well as
 produce the proteins that will carry out the bulk, represents more the
 state a cell is currently in and functions that need to be performed at
 a given time and in a given context.
 While there are housekeeping genes whose expression will not change fundamental
ly between different tissue types and situations that a cell is in, the
 specific RNAs whose proteins will mediate specific functions such as cell
 division or response to a stimulus will.
 It can thus be argued that the sum of RNAs (the transcriptome), or more
 specifically mRNAs provides an overview of the state a cell is currently
 in.
 Hence, measuring those RNAs will allow us to get a picture of the processes
 going on in a cell at a given time.
 The two ways by which this is usually done are either microarrays or RNA
 sequencing, as outlined below.
\end_layout

\begin_layout Subsubsection
Microarrays
\end_layout

\begin_layout Standard
The classic way to measure RNA expression is using microarrays.
 These are, in the simplest case, a carrier matrix (glass can be used but
 the more recent chips all feature a polymer matrix) that has fixed single-stran
ded oligonucleotides spotted on them.
 For our sample, we would then extract and clean the RNA while simultaneously
 degrading any DNA that it contains, and label the RNA either with one dye
 (if we want to quantify the expression levels of transcripts in a given
 sample) or two different dyes (if we want to quantify the difference between
 two samples).
 Such fluorescent dyes used are e.g.
 Cy3 and Cy5, that emit light in the green and red wave lengths upon stimulation
, respectively.
 Note that if the technical reproducibility of the platform is good enough,
 it is also possible to compare conditions by using two different arrays
 where we measure each sample individually and then compare the outcome.
 - A technique that has been favoured by companies like Affymetrix as it
 allowed chips at much higher density and the comparison between each sample
 in a control- and in a experimental condition, as well as between them.
\end_layout

\begin_layout Standard
An obvious drawback of this technology is that we need to know in advance
 which oligonucleotides to spot, so in turn which genes we are looking for.
 There needs to be different chips for different organisms, depending on
 the gene sequences that they carry.
 This selection of genes that we look at can of course introduce bias, in
 such that we can't look for transcripts that we don't know exist, but also
 we might focus on transcripts and isoforms that we think are important
 before carrying out an experiment.
 Another possible issue is their detection threshold.
 As the readout is fluorescence based and there will always be a base-line
 level of it, it is hard to quantify the amount of RNA bound on a spot of
 the total number of molecules are so low that a few molecules do not significan
tly change the readout.
 Turning this argument around, RNA binding to spots also has its point of
 saturation, where it does not matter if there is more RNA present or not
 once all probes bind to their complementary sequence and thus produce a
 signal.
\end_layout

\begin_layout Subsubsection
RNA-seq
\end_layout

\begin_layout Standard
An alternative approach that has become more popular recently with the falling
 costs of DNA sequencing is to apply this technology to RNA as well.
 In this case, we can use the RNA that we isolated from a sample, reverse-transc
ribe it into DNA, and sequence this DNA like we would normally.
 This has got the advantange that it can be used even if there is no genome
 sequence or chip available where as a consequence we are also not biasing
 or selection of genes by previously knowing what to look at, with the exception
 of sections in the RNA that can be more or easily transcribed into DNA
 (like GC-rich regions that also pose challenges for sequencing).
 Another advantage is that the dynamic range this technology is able to
 quantify transcripts is much higher when looking at the reads it produces
 than it is for microarrays, both on the lower as well as on the upper bound:
 we can detect a single read, but also having a lot of reads won't saturate
 our reads the same way that spots on the microarray do (although if a large
 proportion of the reads is from a highly abundant molecule, this will decrease
 sensitivity for detecting other RNAs).
\end_layout

\begin_layout Standard
The way RNA-seq quantification is usually done is to align the reads to
 a known genome, allowing gaps in the alignment for splicing and differential
 use of exons.
 The transcription level of a gene, transcript or exon are then quantified
 by how many reads align to the corresponding section in the genome.
 As this is computationally very expensive, there have been efforts recently
 to get around performing the alignment step by k-mer counting and so-called
 pseudo-alignments.
 Methods for the latter include Sailfish [*], Salmon [*], or Kallisto [*].
\end_layout

\begin_layout Standard
The data produced by RNA-sequencing is different to the one from microarrays,
 as it is based on read numbers that thus counts (so discrete data) instead
 of the continuous fluorescence signal that microarrays provide.
 These counts are well-known to be negatively binomially distributed, and
 require specialised software packages to call e.g.
 differential expression (the most well-known of which are edgeR, DEseq,
 and DEseq2) or transformation before they can be used in standard linear
 modeling techniques (for instance provided by voom in the limma package).
\end_layout

\begin_layout Subsection
Protein and Phosphorylation
\end_layout

\begin_layout Itemize
MS, RPPA
\end_layout

\begin_layout Itemize
phospho = closest to activity from what we have
\end_layout

\begin_layout Subsubsection
RPPA
\end_layout

\begin_layout Subsubsection
Mass Spectroscopy
\end_layout

\begin_layout Subsection
Protein Interactions
\end_layout

\begin_layout Itemize
with DNA
\end_layout

\begin_layout Itemize
with other proteins + different types + different networks
\end_layout

\begin_layout Section
Computational Methods
\end_layout

\begin_layout Itemize
bio moving to quant sci
\end_layout

\begin_layout Itemize
Treatment by Tissue of Origin
\end_layout

\begin_layout Itemize
stratification for prognosis and treatment
\end_layout

\begin_layout Itemize
understanding molecular mechanisms
\end_layout

\begin_layout Itemize
[lopez-bigas: in silico perscription]
\end_layout

\begin_layout Standard
Over the last decades and years, biology moved from an observational, qualitativ
e science to a very much quantitative one.
 This can largely be attributed by high throughput assays, led by the advance
 of DNA sequencing and followed by its derivates as well as other approaches,
 like high-content phenotypic screening - each allowing for measuring not
 one data point, but hundreds and sometimes thousands at a time.
 The magic component in this process is automation: the bulk of the data
 generated by scientific laboratories are not longer carried out by individual
 scientists pipetting together reagens, but by machines controlled by computers
 and designed by both scientists and industry alike.
 This shift of paradigm has not only produced much more data, but arguably
 led to an increase in the quality of such data as well, by removing the
 human element (mood, ability to concentrate, etc.) from large parts of the
 outcome of an experiment.
\end_layout

\begin_layout Standard
This transformation has not only come with its benefits, but also with its
 challenges.
 Setting aside the view of some prominent scientists that each experiment
 needs to have a specific question or hypothesis in mind and that generating
 data first and using the data itself as a hypothesis generator is an inferior
 approach (as best demonstrated by Sidney Brenner's quote 
\begin_inset Quotes eld
\end_inset

low input, high throughput, not output
\begin_inset Quotes erd
\end_inset

), a biomedical scientist's skill requirements have moved from knowing,
 for instance, as many details about a given gene as possible to understanding
 how to set up reproducible assays, generate reliable data, but especially
 treating the resulting data in a way that yielded biological insights.
\end_layout

\begin_layout Standard
There are two crucial parts to this: one is algorithms, that transform the
 raw data generated by a given experiment into biologically interpretable
 indications, and the second is statistics, to make sure the effect observed
 is due to the data and not due to random chance while analysing the results.
 I will outline some very successful ideas that in turn led to algorithms
 and usable software packages for invesitating molecular data, both in terms
 of cancer as well as in general, in the sections of this chapter.
\end_layout

\begin_layout Standard
However, first let us reconsider the goal we are trying to achieve in terms
 of this thesis as well as a lot of the related work: we ultimately want
 to improve how cancer is diagnosed and treated for patients, and those
 algorithms help us to define markers of their pathogenesis or how they
 could reap benefits from being offered a particular treatment.
 We thus want to transform the numbers comprising the molecular data we
 have in a meaningful way so they connect to this endeavour.
\end_layout

\begin_layout Standard
...much has been done, more is missing...
\end_layout

\begin_layout Subsection
Patterns of Mutations
\end_layout

\begin_layout Itemize
biomarkers for drug response
\end_layout

\begin_layout Itemize
long tail of mutations
\end_layout

\begin_layout Standard
The concept of a cell being unable to acquire two mutations that together
 have a damaging effect up to the level of cell death has been termed synthetic
 lethality and has been used to study genetic interactions in model organisms
 for a long time [*], where the loss of both genes in a synthetic lethal
 pair is, as the name suggests, lethal to the organism, but the loss of
 each individual gene is not.
 More recently, these interactions have been used to infer drug sensitivity
 in cancer by mapping orthologous genes to human [*], by using evolutionary
 patterns of metabolic networks [*], or by identifying si/sh-RNA probes
 that are purified from a starting population [*].
 These interactions can then be transformed in a network of synthetic lethal
 pairs [Jerby-Arnon2014], which can then be used to predict sensitivity
 of cancer cell lines to a certain drug treatment, as well as clinical outcome
 given the level of co- expression of its pairs.
 Examples of such pairs are a VHL LoF mutation that is associated with increased
 sensitivity to not previously associated with cancer as shown in their
 article, or the well-known BRCA1/2 loss of function and PARP inhibitors
 [*].
\end_layout

\begin_layout Standard
As all things in biology, things (a) need a p-value and (b) are a bit more
 complicated than they first seem.
 It is such that synthetic lethal pairs can be conditional to a third factor
 (the kind of mutation, a co-ocurring germline variant, a certain environment
 that it is valid in, etc.) that in addition to making sure that the effect
 observed is not due to coincidence alone, examples of gene pairs can not
 be determined with absolute certainty.
 We thus need a way to compare the observed distribution to the one expected
 by chance and quantify our level of certainty about it.
 Unfortunately, there is no easy analytical solution for this.
 Fortunately, there has been some amount of work in defining the right backgroun
d set to compare our observed instance to using numerical simulations.
 To obtain one model sampled from our background distribution, it is not
 enough to randomize the presence or absence of mutations in a matrix â€“
 the degree distribution of both the occurrence of a mutation per gene as
 well as the occurrence of a mutated gene per sample need to be taken into
 account.
 In order to do this, a number of authors developed random versions of presence-
absence matrices, and more Ciriello et al.
 transformed the problem into one of switching edges on a bipartite network
 that proved favorably by means of computational resources required.
\end_layout

\begin_layout Standard
So how do we take advantage of the recent advances in cancer pharmacogenomics
 with regard to co-occurence and mutual exclusivity analysis?
\end_layout

\begin_layout Standard
First, use the already experimentally verified synthetic lethal pairs.
 Then look at them a bit more critically than the original authors who wanted
 to get their results published: Does the change in drug response by leveraging
 on a mutation or an ME pair actually correspond to a treatment difference
 that is applicable, or does it just make a cell line that was resistant
 to a drug a bit less resistant but not in the order of magnitude where
 it would actually be usable? Are we using drugs at concentrations where
 the corresponding ED50/LD50 in vivo is still acceptable? Is the effect
 observed true for only the context that the authors investigated or is
 it more generally applicable? If these conditions are met, candidates and
 candidate combinations should be validated using the appropriate platforms
 and depending on this picked up by pharma companies for further development.
 Second, combine the idea of ME with the idea of network drugs and polypharmacol
ogy.
 One way to reconcile ME pairs with perturbations being propagated in a
 cell's signaling network is to study the dynamic nature of inhibition both
 genes in a pair as opposed to either gene individually.
 Is there a common principle that we can distill in order to get a novel
 kind of understanding in how to rationally design drug therapies? Another
 aspect of this is to study the difference between a knockdown approach
 targeted at the gene to the inhibition of a protein's activity, as the
 mapping between them is not necessarily one-to-one.
 Inhibiting the latter may not reflect the full nature of the protein being
 absent entirely.
\end_layout

\begin_layout Subsection
Gene Expression Clustering
\end_layout

\begin_layout Itemize
mostly subtype clustering
\end_layout

\begin_layout Itemize
heterogeneity
\end_layout

\begin_layout Itemize
global clustering
\end_layout

\begin_layout Itemize
Visualising global expression patterns: PCA, t-SNE
\end_layout

\begin_layout Standard
Cancer is known to be a heterogeneous disease, with individual tumours forming
 different subtypes within the same tissue.
 In turn, cells in the same tumour mediate different functions essential
 to its survival and growth.
 One of the arguably most effective ways of eludicating the state of individual
 tumours and cell populations therein is its transcriptome, where transcription
 of individual genes and regulatory RNAs can be assessed using methods such
 as microarrays or RNA sequencing.
\end_layout

\begin_layout Standard
Disregarding the effect of different cell types (by making the assumption
 that we sampled a tumour evenly and uniformly while excluding neighboring
 non-cancerous cells, which is not really the case), we can use the global
 pattern of gene expression to identify different subtypes of the disease
 in a given tissue (or even between tissues).
 A challenge with this is is that the transcriptome could be comprised of
 up to 22,000 genes without even considering most regulatory or ribosomal
 RNAs.
 This space needs to be reduced to something more manageable for visual
 inspection, as well as different subgroups within assigned that have prognostic
 or therapeutic relevance.
 Approaches the allow for visual inspection of a high-dimensional data set
 are called dimensionality reduction techniques, while methods assigning
 the different samples to different subtypes are called clustering algorithms.
\end_layout

\begin_layout Subsubsection
Visualising Patterns in High-Dimensional Data
\end_layout

\begin_layout Standard
Principal Component Analysis (PCA).
 PCA is one of the simplest linear transformations that rotates samples
 in N-dimensional space (where N is the number of observations per sample)
 in a way that its projection to M-dimensional space (the target dimensions
 - for interpretability usually two or three) maximises the variance contained
 in M.
 The rotation is calculated using the matrix factorisation [add fact here],
 which can be simplified to [add here] and yields a unique solution.
 [params] provide the position of a sample in M space where the axes are
 principal components, and the latter's projection back into N called loadings
 (which represent how much of each original axis is contained within the
 new axes).
 It is important to note the meaining and interpretability of such a decompositi
on is strongly dependent on the amount of variance captured in the reduced
 space.
 PCA is closely related to Singular Value Decomposition (SVD), which has
 applications in dimensionality reduction as well.
\end_layout

\begin_layout Standard
T-distributed Stochastic Neighbor Embedding (t-SNE).
 Instead of relying on the most variable global structures, this method
 visualises local structures in a given data set: Starting from a point
 in space (the sample), additional samples are distributed in its vicinity
 depending on their distance to the original sample, as well as the other
 points considered (that are a subset of the total number of points).
 Using this method, one can only trust the points neighbouring each other
 up to a certain number for which they were considered, as specified by
 the perplexity parameter.
 This method has been re-branded as vi-SNE [cite] by Dana Pe'er in their
 cyt software tool [cite], and later extended by the original author to
 use the Barnes-Hut method [cite] that is usually used in large N-body simulatio
n in astrophysics.
\end_layout

\begin_layout Subsubsection
Clustering
\end_layout

\begin_layout Standard
K-Nearest Neighbors.
 This is one of the simplest clustering algorithms that requires the number
 of clusters to be known a priori.
 It starts with assigning N cluster centres randomly in a given data set,
 and then assigning all samples that have a smaller distance to a given
 centre than the others to that centre.
 These assignments are iterative, which means that once the cluster centres
 have been determined and the samples assigned, the centres are updated
 to correspond to the centre of the samples each cluster is associated with.
 In turn, which samples are associated with which clusters is also updated
 after each time clusters get new samples assigned, until the process converges
 and further updating steps do not change cluster assignments anymore.
 This method is not used in this work.
\end_layout

\begin_layout Standard
Non-Negative Matrix Factorisation (NMF).
 As the name already suggests, this is another matrix factorisation method
 that decomposes the matrix V (usually with observations in rows and samples
 in columns) into the matrices W (with samples in columns and weights for
 each cluster in rows) multiplied with the matrix H (cluster assignment
 vectors in columns and samples in rows).
 It does not determine the optimal number of clusters by itself, but can
 be run with different numbers of clusters that is later evaluated using
 the cophenetic coefficient (a measure of goodness of fit of the samples
 to the cluster centres).
\end_layout

\begin_layout Standard
Latent Dirichlet Allocation.
 A modern, Bayesian method for clustering that is an active research topic
 but not used in this work.
\end_layout

\begin_layout Subsection
Differentially Expressed Subnetworks
\end_layout

\begin_layout Itemize
usually in the interactome
\end_layout

\begin_layout Itemize
PCSTs (BioNet), etc.
\end_layout

\begin_layout Standard
Instead of or in addition to investigating global differences in the patterns
 of gene expression, one might be interested in how a subset of interacting
 proteins differs between known groups of samples.
 
\end_layout

\begin_layout Subsubsection
Minimum Spanning Trees
\end_layout

\begin_layout Standard
The problem of finding a differentially expressed connected subnetwork in
 the network of interacting proteins is similar to the problem of a Minimum
 Spanning Tree, that is, selecting only edges and nodes in a graph that
 maximise a score where each nodes adds to and each edge subtracts from
 it.
 
\end_layout

\begin_layout Subsection
Gene Regulatory Networks
\end_layout

\begin_layout Standard
An important aspect of gene expression is that its patterns are not randomly
 but hierarchically organised.
 On perturbations on a cell's surface or its interior a signal is propagated
 from its origin up to proteins that bind to DNA and change their expression
 as a response to the stimulus (more on this in the next section).
 The terminal nodes of this signal transduction are called transcription
 factors, that upon binding on the DNA mediate and direct (either in a promoting
 or in an inhibiting fashion) binding of DNA-dependent RNA polymerase (PolII
 [?]) that forms a complex with available factors in order to start transcrition
 of a factor's target genes.
 The genes transcribed upon activation of transcription factors may in turn
 be transcription factors themselves that cause increased or decreased transcrip
tion of other genes.
 These interactions between different transcription factors are usually
 referred to as a Transcription Factor Network or Gene Regulatory Network.
\end_layout

\begin_layout Standard
There are multiple ways these can be investigated: by characterising which
 transcription factors bind to which genes (which can be done either experimenta
lly, as outlined in section *, or computationally by looking at which sequence
 of nucleotides a given factor is likely to bind), by finding genes that
 change in a coordinated fashion and hypothesise that those might be regulated
 by the same set of factors, or by making inferences which combination of
 which factors is required for a certain gene to be transcribed.
\end_layout

\begin_layout Subsubsection
Binding Motifs
\end_layout

\begin_layout Itemize
JASPAR, TRANSFAC
\end_layout

\begin_layout Standard
Some, albeit not all, transcription factors prefer to bind a specific sequence
 of nucleotides that the recognize on the DNA strands.
 Depending on how strong this preference is, it may require a certain amount
 of experiments to find a statistical enrichment in the regions it was found
 to bind in terms of the nucleotide sequences that peaks are found in ChIP-seq.
 This preference for a certain string of nucleotides is called a motif.
 The enrichment in ChIP-seq peaks can be found using tools such as MEME
 [*] that will report different weight matrices for nucleotides around the
 binding region (which correspond to how often a given nucleotide is found
 in a given position).
 These motifs in turn can be used to scan the genome for the same or a similar
 sequence using a sliding window approach that may indicate sites a transcriptio
n factor could bind but did not in the original experiment.
 Such can for instance be the case if the chromatin is too densely packed
 in a region (but may not be if the cell was in a different state or from
 a different tissue), or the binding site is occupied by another transcription
 factor blocking the binding of the one we are looking at.
 There are databases that collect and store those motifs from experimental
 data and their known and predicted binding in different cell types, such
 as JASPAR [*],TRANSFAC [*], or the Ensembl Regulatory Build [*].
\end_layout

\begin_layout Subsubsection
Mutual Information for Transcription Factor Networks
\end_layout

\begin_layout Itemize
ARACNE
\end_layout

\begin_layout Itemize
TF logic
\end_layout

\begin_layout Standard
Another question that we might be interested in is which genes share a common
 regulator.
 In the simplest case this is a transcription factor that, upon activation,
 transcribes a set of genes in a coordinated fashion.
 Target genes of this transcription factors may act in that capacity themselves,
 so we come back to the construction of a GRN.
 Turning the argument on its head, we can look genes that are expressed
 in a coordinated fashion across different conditions (which includes e.g.
 different patient samples, as long as we can assume that the transcription
 factor-target relationship is the same) by calculating the mutual information
 of all gene pairs, setting a lower bound to consider, and the pruning the
 edges in the resulting network by postulating that a link between genes
 A and C is indirect if the mutual information of A and B, as well as B
 and C is higher than the one of A and C (a concept called Data Processing
 Inequality).
 On top of ARACNE, the authors also propose a method they call Master Regulator
 Analysis (MRA) [basso, nat.gen] to identify the regulatory network in human
 B cells.
\end_layout

\begin_layout Standard
In an extension proposed by the same group is to condition the mutual informatio
n on the expression of a modulator that they call MINDy.
 The idea here is to look at the upper and lower third of the modulator
 expression, and calculate mutual information for both of theses sets.
 Afterwards, instead of inferring the network, the authors look for the
 strongest changes in mutual information between the two subsets: if that
 is the case, the modulator can be seen as a cofactor required for transcription
 factor regulation.
 If a high modulator expression corresponds with increased mutual information
 that will likely be activating, or inactivating if in corresponds to decreased
 mutual information.
\end_layout

\begin_layout Standard
In another extension, they proposed a method call Master Regulator Inference
 Analysis (MaRInA), which ***
\end_layout

\begin_layout Subsection
Signaling Aberrations
\end_layout

\begin_layout Itemize
Transcription: DNA->RNA, RNA data
\end_layout

\begin_layout Itemize
signaling/via TFs points
\end_layout

\begin_layout Itemize
proteins
\end_layout

\begin_layout Itemize
Cell Signaling: actual activity
\end_layout

\begin_layout Itemize
pathway expression vs activity
\end_layout

\begin_layout Standard
The transduction of a signal from a cue either binding to a receptor on
 a cell's surface or intracellularly, is how a cell responds to changes
 in its environment.
 In turn, this signal is relayed through the network of kinases and phosphatases
 (there are other chemical modifications, but those are the best studied)
 until it reaches the terminal nodes that are the transcription factors,
 acting in conjunction with polymerase III and other cofactors to initiate
 changes in gene expression.
 Those genes are in turn transcribed into RNA, and if they are protein-coding
 consequently translated into those.
 The process of transcription and translation is referred to the central
 dogma of molecular biology.
\end_layout

\begin_layout Standard
Cell signaling is deregulated in many diseases, including cancer that we
 focus on here because of the sheer wealth of available data.
 But how to best quantify the signaling activity? The closest proxy we have
 would be to quantify post-translational modifications that are known to
 confer activity.
 In the simplest case this could be a phosphate group on a certain position
 that is known to make a kinase active, that is that it in turn phosphorylates
 its downstream targets.
 But phosphorylation data, and to a lesser extent protein data in general
 is much harder to come by than sequencing data.
 Mass spectrometry and Reverse Phase Protein Arrays just do not produce
 the same clarity that a DNA sequence or RNA level provides, plus is much
 harder to generate because the technology that could to it with the same
 throughput as sequencing technologies just does not exist.
\end_layout

\begin_layout Standard
We can thus argue that computational methods are required to make statements
 about cell signaling, starting from DNA and RNA instead of protein and
 PTM data.
 The simplest approach to this is looking how much mRNA of signaling molecules
 are expressed that comprise a pathway, and designate a high expression
 a high activity and vice versa.
 This, however, is at odds with the way cell signaling works and is regulated.
 Inferring the protein levels from mRNA levels may indeed be viable [60%
 paper], but two steps removed from the PTMs.
 Methods have been developed to address the issue of taking the expression
 level of a gene set as activity proxy by considering the structure and
 signs of the different pathway molecules, e.g.
 Signaling Pathway Impact Analysis [ref] or Pathifier [ref].
 They, however, do still not distinguish between expression level and activity.
 Another method, PARADIGM [ref] could in theory support it, depending the
 pathway structure supplied for inference.
 Yet, the focus has never been to tell apart activity from expression.
 I elaborate on these issues further in chapter 4, with a possible way to
 resolve them.
\end_layout

\begin_layout Subsection
Signature Matching
\end_layout

\begin_layout Itemize
Old Connectivity Map
\end_layout

\begin_layout Itemize
requirement for suitable experiments
\end_layout

\begin_layout Standard
When investigating the effect small molecules (i.e., drugs) have on a system,
 we might not want to look at the mechanism by which this happens at all,
 but instead rely on the changes of gene expression upon treatment with
 it.
 Then we could use the downstream genes that are changing as a signature
 of treatment with that drug.
 This has got two applications that have indeed been used, which are the
 following: (1) matching a drug's signature with another drug's signature,
 and if they are a close match but have e.g.
 different indications, suggest that each drug may be used for the other
 indication (where directionality may be limited by additional factors),
 or (2) matching a drug's signature with the inverse gene expression changes
 that a disease exhibits over normal controls, we can suggest that this
 drug may be used to counteract the effects of the disease [MANTRA, DvD],
 i.e.
 be a potential treatment if the disesase is indeed caused by the gene expressio
n changes that the drug reverses.
 Of course, those matches should rather be seen as hypothesis generators
 than definite indications for repurposing and treating diseases.
\end_layout

\begin_layout Standard
Of course, this kind of approach relies on the availability of signatures
 of the drugs we look at, or of a given drug and disease, respectively.
 That first large-scale project providing signatures of drug-perturbed gene
 expression changes in the MCF-7 cell line was the Connectivity Map [Lamb2006],
 which was subsequently extended and the current version features 1.4 million
 conditions where 978 transcripts are quantified using the L1000 platform
 - more on this in chapter 3.
\end_layout

\begin_layout Section
Reproducibility of Results
\end_layout

\begin_layout Subsection
Experimental and Computational
\end_layout

\begin_layout Itemize
focussing on comp reproducibility here
\end_layout

\begin_layout Itemize
methods desc inadequate -> need code
\end_layout

\begin_layout Itemize
also this thesis will inadequately describe what was done
\end_layout

\begin_layout Itemize
more data available
\end_layout

\begin_layout Itemize
comp analyses get more complex
\end_layout

\begin_layout Standard
Reproducibility of scientific experiments has recently gotten into the spotlight
 when [someone] published a high-profile study that tried to independently
 validate findings of [40?] findings reported by different groups in the
 journals Nature, Science and Cell.
 For half of the studies that they invesigated, they could not obtain the
 same results and thus conclusions that the respective authors reported.
 Needless to say, this raised some concerns about the current paradigm of
 publishing new results quickly instead of thoroughly and overclaiming the
 effect or significance of a study in order to have an article accepted
 in the top tier journals.
 Follow-up studies have pointed a part of the blame on use of different
 antibodies [nat news+refs] that did not always show the affinity and specificit
y to their target as the vendors claimed, which has resulted in the creation
 of a registry for validated antibodies at [somewhere].
\end_layout

\begin_layout Standard
It is easy to argue that experimental reproducibility is a hard issue to
 tackle, as the potential for confounding variables is huge and a lab will
 hardly ever have the resources to account for all of them.
 In general, good study design, that is proper controls and randomisation,
 should go a long way (especially in the case of clinical trials), but even
 then, there may be confounding effects unknown to the experimenters and
 not properly controlled for, as has been shown in the case where male vs.
 female stewards in a mouse facility produced different reactions of the
 animals, potentially influencing the results obtained from a very large
 number of studies.
\end_layout

\begin_layout Standard
Apart from experimental reproducibility, computational reproducibility should
 be an issue that is easier to tackle, because at least for deterministic
 algorithms the same input should always produce the same output using the
 same tools.
 However, this is also easier said than done because the tools will depend
 on other tools, maybe with different versions, and each version may change
 the way they treat the data to go from input to output in a slighly different
 way.
 This is especially true with computational analyses, as well as the tools
 employed, are getting more and more complex.
 However, even if the potential confounding factors are smaller than for
 experimental scientists, computational analysis also has its high-profile
 fraught with issues.
 The most well-known example of this is maybe Anil Potti's cancer gene signature
s [*] that Keith Baggerly later spent six months trying to reproduce - and
 failed, but in the process found considerable errors that later caused
 the original article to be retracted [*].
\end_layout

\begin_layout Standard
To summarise, ensuring that a subsequent study has the possibility to arrive
 at the same conclusions given the same starting point, and in turn build
 on the results in a more confident way, has gotten a big enough issue to
 dedicate an introductory chapter to it.
 Since I did not perform any experiments to produce data myself, the focus
 of this chapter shall be to ensure that the results obtained by transforming
 raw (or in some cases already processed) data into other types of data,
 plots, and ultimately interpretation are reproducible in a sense that a
 person not involved in the projects could arrive at the same conclusion,
 being provided this thesis, the code used, and the technical documentation
 written in conjunction with it.
 This is especially true because the maybe best known retraction caused
 by computational irreproducibility handled a top very similar to the one
 I investigate in this thesis: the effect of signaling pathway signatures
 of different cancers (chapter X) and their significance (chapter Y).
\end_layout

\begin_layout Subsection
Scientific Software Ecosystem
\end_layout

\begin_layout Itemize
programming language: not too high-/low-level
\end_layout

\begin_layout Itemize
availability of support packages: huge progress with R, python
\end_layout

\begin_layout Standard
With computational analyses getting more complex, it is no longer enough
 to just apply a simple statistical test for a number of obervations of
 one condition vs.
 another.
 The wealth of data that has become available needs preprocessing, normalising,
 statistical analysis, and interpretation of results.
 No one scientist can perform all of these tasks completely independently,
 as some of them may detailed knowledge of all the algorithms involved,
 up to an extent that is prohibitory.
 It is thus required to package repeated steps together in a higher-order
 functionality.
\end_layout

\begin_layout Standard
This is where the scientific software ecosystem comes in.
 And, to a larger extent, the ecosystem of general software.
 For instance, I want to obtain, preprocess, and normalise microarray data
 to then compute differentially expressed genes in two conditions.
 The concepts of obtain, preprocess, and normalise are well enough defined
 that I should not have to worry about their exact implementation.
 I just need to know what these concepts mean and there is a software that
 abstracts the low-level implementation to a higher-level function that
 I can just apply.
 And, in addition, I want to script those steps together without needing
 to worry about things like the exact memory allocation in each step.
\end_layout

\begin_layout Standard
Two programming languages, along with the packages they themselves as well
 as their users provide, have been fundamental in providing a toolset that
 allows processing, exploration, and analysis of the amount of data contemporary
 experiments provide.
 These are R and Python, along with packages hosted on CRAN/BioConductor
 and PyPI, respectively.
 Without those tools, performing analysis like I do to obtain the results
 shown later would not be possible.
\end_layout

\begin_layout Subsection
Documentation
\end_layout

\begin_layout Itemize
roxygen
\end_layout

\begin_layout Itemize
markdown
\end_layout

\begin_layout Subsection
Versioning
\end_layout

\begin_layout Itemize
why keeping old version is important
\end_layout

\begin_layout Itemize
tools to support
\end_layout

\begin_layout Itemize
how github really put it together
\end_layout

\begin_layout Subsection
Reproducible Workflows
\end_layout

\begin_layout Itemize
automation
\end_layout

\begin_layout Itemize
single entry point for analyses
\end_layout

\begin_layout Itemize
Makefiles
\end_layout

\begin_layout Itemize
LIMITATIONS:
\end_layout

\begin_layout Itemize
run -> paper would be nice, but:
\end_layout

\begin_layout Itemize
computing time for large analyses
\end_layout

\begin_layout Itemize
complex software setup req'd, may not be available later
\end_layout

\begin_layout Itemize
figures need manual tweaking (or lot of time)
\end_layout

\begin_layout Itemize
holy grail would be something like shiny
\end_layout

\begin_layout Standard
A challenge that often is underestimated is that even when all of the code
 that was used in order to generate the is provided, it is often not trivial
 know which script generates which part of the analysis, which other scripts,
 analyses, or data it depends on.
 Thus it is not only important to provide the code as it is, but also informatio
n on how to run it, so to not only have all the bits and pieces but to connect
 them to a workflow from the data to the results.
\end_layout

\begin_layout Section
Motivation and Outlook
\end_layout

\begin_layout Itemize
aberrations on cell signaling level, we want to make inferences about that
\end_layout

\begin_layout Itemize
because that's what should be causing our phenotype (drug response or patient
 survival)
\end_layout

\begin_layout Itemize
and it should be better than methods looking at other proxies that represent
 something that is somewhat correlated with it, but not as directly
\end_layout

\begin_layout Standard
Cancer sequencing projects (ICGC, TCGA) are giving us an unprecedented view
 on the mutational landscape of human cancers.
 In addition, this information is augmented by genomic and general molecular
 characterisation of cell lines and their responses to anticancer compounds
 (GDSC, CCLE), or by selection viable genotypes under a selective constraint
 (Achilles projects).
 These projects have led us to believe that, while this landscape is vast,
 mutations tend to occur in a limited set of pathways, often exhibiting
 a pattern different from the ones one would expect by random chance.
 As these modules deviate from a randomly expected distributions, it is
 commonly accepted that they represent the pattern of evolutionary adaption
 that enabled a cancer to obtain its hallmarks (Hanahan2000,2011).
 Mutations that are generally selected for and often functionally characterised
 to be tumorigenic are often referred to as driver mutations, while those
 following the background pattern are likely to occur just by replication-
 and DNA repair defects.
\end_layout

\begin_layout Standard
When looking at sets of mutations instead of individual ones, the occurrence
 of those patterns has been quantified by a couple of approaches, identifying
 modules that are either co-occurring or mutually exclusive.
 In the event of co-occurring mutations, this might mean that a given mutation
 is not sufficient to obtain a certain trait and thus a second mutation
 is necessary to confer it.
 In the case of mutual exclusivity, a second mutation is not conferring
 a growth advantage after the first arose.
 This may be due to evolutionary parsimony or a fitness defect.
 In the first case, a second mutation that inactivates, for example, an
 already inactivated tumour suppressor pathway is unlikely to happen on
 the population-level by the pure fact that it is not selected for, as the
 required trait has already been acquired by the cell.
 In the second case, the growth advantage conferred by one mutation might
 be cancelled out or counteracted by the presence of a second one, thereby
 mediating a selection growth disadvantage (fitness defect) of the cells
 carrying both mutations as opposed to either one of them.
 Much focus has been on identifying driver mutations in different cancer
 types, that, combined with oncogene addition (i.e., the tendency of a transformed
 cell to become dependent on sustained impact of the lesion it first obtained)
 forms the basis of many targeted therapies: if you can inhibit signaling
 stemming from a mutation that drives a cancer's development and progression,
 the affected cells' growth will be severely inhibited.
 Unfortunately, targeted therapies often can not kill all cells before they
 acquire a resistance mechanism to the treatment, or a sub-population of
 cells were resistant to begin with and then outgrow their competitors.
 This is one of the reasons why targeting mutations in driver genes is a
 good start, but needs to be augmented with knowledge of the dynamic changes
 it induces in the cellular signaling network.
\end_layout

\end_body
\end_document
