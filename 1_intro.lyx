#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[font=small,labelfont=bf]{caption}

\newcommand*\name[1]{#1}                                                                                                                                                                                                                       
\newcommand*\abbrv[1]{#1}                                                                                                                                                                                                                      
\newcommand*\rpkg[1]{\textit{#1}}                                                                                                                                                                                                              
\newcommand*\file[1]{\textit{#1}}                                                                                                                                                                                                              
\newcommand{\code}[1]{\texttt{#1}}                                                                                                                                                                                                             
                                                                                                                                                                                                                                               
\newcommand*\protein[1]{#1}                                                                                                                                                                                                                    
\newcommand*\gene[1]{\textit{#1}}                                                                                                                                                                                                              
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Part
Introduction
\end_layout

\begin_layout Section
Cancer Biology
\end_layout

\begin_layout Subsection
Significance and epidemiology
\end_layout

\begin_layout Standard
The International Agency for Research on Cancer estimates of global incidence
 of cancer to be 12.7 million new cases on 7.6 million cancer deaths based
 on estimates for 182 countries in 2008 
\begin_inset CommandInset citation
LatexCommand cite
key "Ferlay2010-an"

\end_inset

.
 In the United States, it has even surpassed heart disease as the leading
 cause of death in people younger than 85 
\begin_inset CommandInset citation
LatexCommand cite
key "Twombly2005-gg"

\end_inset

.
 Breast cancer is the most abundant form in females, accounting for a total
 of 23% of cases and 14% of deaths.
 For males, the most abundant is lung cancer, comprised of 17% of cases
 and 23% of cancer-related deaths 
\begin_inset CommandInset citation
LatexCommand cite
key "Jemal2011-ku"

\end_inset

.
\end_layout

\begin_layout Standard
Needless to say, the disease is a global health concern and better ways
 of diagnosis and treatment are needed, as well as better understanding
 of the molecular mechanisms.
\end_layout

\begin_layout Subsection
Mutational processes
\end_layout

\begin_layout Subsubsection
Germline variation
\end_layout

\begin_layout Standard
In the human population, there is natural variation in the genome from one
 individual to another, which gets passed down through generations in the
 process of reproduction.
 Each human being has got two copies of their DNA, one passed down from
 their mother and the other passed down from their father.
 Disregarding of where the copies that an individual actually inherits came
 from, they may harbour the same sequence (that is, the individual is homozygous
 in a given base or gene) or may be different between those (that is, the
 individual is heterozygous).
 This is (in part) what makes us look different, but also what may cause
 us to be more or less susceptible to a certain disease or treatment thereof.
 Since the genes were assembled in the fertilized egg already that afterwards
 underwent cell divisions (in a process also called mitosis), all the cells
 in our body in theory harbour the same DNA sequence, with the exception
 that sex cells only contain one copy that was assembled in a cut-and-paste
 process (called meiosis) of the two somatic ones.
\end_layout

\begin_layout Subsubsection
Somatic variation
\end_layout

\begin_layout Standard
Should we sequence each cell in an individual, we would not find that all
 of their consecutive bases made up of As, Ts, Cs, and Gs are indeed identical,
 but there also variation within each individual.
 There may be a change that is common to cells that derive from the same
 precursor, or a change may have been introduced in a single cell by an
 internal or external process.
 In the former case, we need to realize that the DNA copy mechanism (also
 called DNA replication) that ensures that that when a cell splits both
 its daughter cells inherit two full copies of its DNA is not infinitely
 accurate, but this process may introduce reading- (from the ancestral or
 template strand) and writing (the newly synthesised strand) errors.
 In fact, in each cell division there is an average of *** errors that are
 introduced while copying the each of the three billion bases (times the
 two alleles) that is our genome.
\end_layout

\begin_layout Standard
Such an error may manifest itself in multiple ways.
 The replication apparatus (DNA polymerase) may miss the insertion of a
 base or a couple of bases that was in the ancestral in the newly synthesised
 strand, which produces a small deletion.
 In contrast, it also may insert a base that was not in the original strand,
 thereby producing an insertion.
 It may also insert the wrong base, which leads to a substitution.
 While discussing theses mutational processes one should keep in mind that
 those errors introduced do manifest itself only on one of the two strands
 in an allele, which will lead to a base not perfectly matching its opposite
 base anymore.
 This means they do not pair as well as they were if there had been no error.
\end_layout

\begin_layout Subsubsection
Mutations, copy number alterations, and structural variation
\end_layout

\begin_layout Standard
Even when the replication process produced a perfect copy of the ancestral
 strands, there may still be cell-internal or external processes that affect
 the DNA's integrity 
\begin_inset CommandInset citation
LatexCommand cite
key "Alexandrov2013-qo"

\end_inset

.
 This could be single base exchanges (single nucleotide polymorphisms, SNPs),
 small insertions and deletions (small indels), changes in the number of
 copies of DNA segments (copy number alterations--this can happen either
 on the genome or on extragenomic small chromosomes 
\begin_inset CommandInset citation
LatexCommand cite
key "Jones2012-fc"

\end_inset

), or strand breaking and rejoining at different positions (structural rearrange
ments--from single events that produce gene fusions 
\begin_inset CommandInset citation
LatexCommand cite
key "Nowell1960-rh,Mitelman1997-au"

\end_inset

 to 
\begin_inset CommandInset citation
LatexCommand cite
key "Korbel2007-nq"

\end_inset

 catastrophic DNA shattering and rearrangements 
\begin_inset CommandInset citation
LatexCommand cite
key "Rausch2012-mt,Northcott2012-ci"

\end_inset

).
\end_layout

\begin_layout Subsection
Consequences
\begin_inset CommandInset label
LatexCommand label
name "sub:Consequences"

\end_inset


\end_layout

\begin_layout Standard
Genes that are relevant for cancer development and progression have long
 been classified in Oncogenes and Tumour Suppressor Genes.
 In broad terms, this classification represents the following: does a mutation
 introduce a gene function that was not there before, or it loses sensitivity
 to an inhibitory signal that kept it in check, this gene is called an Oncogene
 in its active (mutated) form, or Proto-Oncogene in its wild-type form;
 conversely, if a mutation causes a gene a lose its function as e.g.
 a negative regulator of cell growth, it is called an Tumour Suppressor
 Gene.
 The first Tumour Suppressor Gene that has been found was TP53 in ***, now
 termed 
\begin_inset Quotes eld
\end_inset

Guardian of the Cell Cycle
\begin_inset Quotes erd
\end_inset

 for its numerous functions maintaining DNA integrity, managing DNA repair,
 and causing apoptosis or senescence of the former fail.
 More recently, mutations that actively contribute to either development
 of progression of a cancer have been called drivers, while the mutations
 introduced by e.g.
 a faulty DNA replication machinery (that a cancer may have caused) that
 bear no functional impact on the cell are called passenger mutations.
 Driver genes are usually either activated Oncogenes or Tumour suppressor
 genes, but may also have both functions.
\end_layout

\begin_layout Standard
A gene that has an erroneous sequence somewhere on the DNA is, by itself,
 not a cause for a cell to alter its function.
 Instead, genes only store the information, like having a template, that
 is required to form an active compound from it.
 This may be a regulatory RNA, or a messenger RNA (mRNA; both derived from
 DNA by a process called transcription) that is later used by the Ribosome
 to chain amino acids together to a functional protein in a process called
 translation.
 The path from mRNA to protein may involve additional steps, such as cutting
 out unneeded parts or pasting together different building blocks (splicing),
 or covalently attaching sugars or other chemical groups (post-translational
 modifications) to yield the functional protein and/or to regulate its activity.
\end_layout

\begin_layout Standard
Take, for instance, a protein is involved in relaying a signal from the
 cell surface, such as a signal to grow and divide, to another protein that
 in turn activates some other proteins which ultimately induces the expression
 of genes that are needed for the cell's replication machinery.
 These signals are usually relayed by post-translational modifications from
 one protein to another.
 One of the best-studied processes is that of phosphorylation (the addition
 of a phosphate group - proteins that do that are called kinases, the ones
 that remove phosphatases) on Serines or Threonines (that have an OH group
 in their side chain and are thus susceptible to it).
\end_layout

\begin_layout Standard
This protein now may be affected by a mutation so that it no longer waits
 for an upstream signal to transduce, but sends the downstream signal to
 grow and divide irrespective of what the upstream input is.
 One such example is the well-known 
\begin_inset Formula $BRAF^{V600E}$
\end_inset

 mutation, where a Valine on position 600 (the 600th amino acid) is replaced
 by Glutamic Acid.
\end_layout

\begin_layout Subsection
The path to uncontrolled growth
\end_layout

\begin_layout Standard
Mutagenic driving forces are not only all around but also inside us (as
 exemplified by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gene{APOBEC}
\end_layout

\end_inset

 virus defence 
\begin_inset CommandInset citation
LatexCommand cite
key "Alexandrov2013-qo"

\end_inset

).
 However, most cells do not start to divide uncontrollably, even if they
 acquire a driver mutation.
 For instance, it has been shown that in healthy skin there is a high number
 of potential driver mutation that pre-exist without cancer ever developing
 from them 
\begin_inset CommandInset citation
LatexCommand cite
key "Martincorena2015-iw"

\end_inset

.
 For a cell to develop into a malignant tumour, it requires multiple 
\begin_inset Quotes eld
\end_inset

hits
\begin_inset Quotes erd
\end_inset

 that transform it into a truly malignant state.
 The mutations and mechanisms but which it acquires those properties are
 different from cancer to cancer, yet the biological processes it needs
 to modify are remarkable similar 
\begin_inset CommandInset citation
LatexCommand cite
key "Hanahan2000-is,Hanahan2012-im"

\end_inset

:
\end_layout

\begin_layout Itemize
Self-sufficiency in growth signals
\end_layout

\begin_layout Itemize
Insensitivity to anti-growth signals
\end_layout

\begin_layout Itemize
Evading apoptosis
\end_layout

\begin_layout Itemize
Limitless replicative potential
\end_layout

\begin_layout Itemize
Sustained angiogenesis
\end_layout

\begin_layout Itemize
Tissue invasion and metastasis
\end_layout

\begin_layout Itemize
Avoiding immune destruction
\end_layout

\begin_layout Itemize
Deregulating cellular energetics
\end_layout

\begin_layout Section
Therapeutic interventions
\end_layout

\begin_layout Subsection
The therapeutic window
\end_layout

\begin_layout Standard
Within a therapeutic intervention, our goal is to selectively treat (or
 kill) the diseased cells while not impacting the normal function of other
 cells in the body.
 If at first we assume that a treatment can be delivered to all cells of
 the body in the same amount, the question is whether the impact it has
 is impacting the cells we want to act on more than all the other cells
 we want to impact as little as possible.
 If we take the simple example of killing cells, we want a drug that kills
 all the bad cells and leaves the good cells alone.
 An effective drug will work on the bad cells at a lower concentration than
 on the good ones.
 If the concentration is too high, it will undoubtedly affect other cells
 as well.
 This range of concentration, where a given drug already acts on the target
 cells but does not confer toxicity to other cells is called the therapeutic
 window.
 The broader it is, the easier it is to work with this drug.
\end_layout

\begin_layout Standard
Elaborating on our simplification, it is of course not the case that an
 administered compound will be available to all cells at the same concentration.
 Instead, it first needs to reach the bloodstream (which includes uptake
 and resorption, then modification in the liver [*?] and pancreas if taken
 orally), then be distributed by the flow of blood into all the capillaries
 (which the exception of the brain that has an additional barrier), until
 finally cells are able to absorb it.
 As a rule of thumb whether a drug can be orally absorbed or not, a common
 measure is 
\begin_inset Quotes eld
\end_inset

Lipinsky's Rule of Five
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Lipinski2012-cl"

\end_inset

, that states that an orally effective drug has no more than one violation
 of (1) no more than five hydrogen bond donors, (2) no more than ten hydrogen
 bind acceptors, (3) a molecular mass less than 500 daltons, (4) an octanol-wate
r distribution coefficient (
\begin_inset Formula $logP$
\end_inset

) of less than five.
\begin_inset Foot
status open

\begin_layout Plain Layout
Note that these are only four and not five rules.
 The 
\begin_inset Quotes eld
\end_inset

five
\begin_inset Quotes erd
\end_inset

 in the rule's name stems from the components being multiples of five, not
 that there are five rules
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cytotoxic drugs
\end_layout

\begin_layout Itemize
based on cells growing quicker/consuming more resources
\end_layout

\begin_layout Standard
...put some examples of clinical cytotoxic here...
\end_layout

\begin_layout Subsection
Targeted therapies
\end_layout

\begin_layout Standard
A recurrent theme of cancer development and progression is the aberrant
 activation of specific molecular cues in cell signaling.
 An example of this is the well-known 
\begin_inset Formula $BRAF^{V600E}$
\end_inset

 mutation, already mentioned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Consequences"

\end_inset

.
 But there are many more instances where a mutation in a gene yields a gene
 product that is abnormally active- or inactive.
 This often includes members of the MAP kinase pathway (like EGFR, RAS,
 RAF, MEK, or ERK), but frequently - and often strongly correlated with
 the general cell state as is defined by, for instance, the tissue of origin
 - other proteins and pathways that confer a selective advantage in a given
 context.
 An important aspect in that regard is the concept of oncogene addiction:
 Once a cell, or a population of cells, suffers a molecular lesion that
 causes aberrant signaling, the cell (or cells) become dependent on exactly
 that signal.
 Turn it off, and those cells are likely to die.
\end_layout

\begin_layout Standard
This has an important implication for cancer therapy: if we are able to
 find (or design) a compound that specifically turns off one of those abnormally
 active proteins, we can kill cells that have it active.
 
\end_layout

\begin_layout Standard
Coming back to our example of the 
\begin_inset Formula $BRAF^{V600E}$
\end_inset

 mutation, [* et al] indeed designed and developed a small-molecule inhibitor
 called Plexicon (more specifically, PLX4720) that proved to bind the mutation
 version of the BRAF protein with a much higher affinity than the one found
 in wild-type cells.
 This, in some ways, could be considered the perfect drug for cancer therapy,
 because it has it has favourable uptake in the body [*], and once it is
 there the therapeutic window is much larger compared to other compounds
 due to this difference in binding affinity.
\end_layout

\begin_layout Standard
There are, however, many other kinase inhibitors as well as antibodies that
 specifically target a protein to abrogate its activity.
\end_layout

\begin_layout Subsection
Immune therapy
\end_layout

\begin_layout Itemize
PD-1, PD-L1 inhibitors
\end_layout

\begin_layout Standard
When the President of the United States announced the so-called Moonshot
 effort that promised funding for developing and improving personalised
 cancer treatment by targeted therapies, immune therapies were set to play
 a profound role in it.
\end_layout

\begin_layout Subsection
Development of resistance
\end_layout

\begin_layout Standard
Unfortunately, targeted therapies often can not kill all cells before they
 acquire a resistance mechanism to the treatment, or a sub-population of
 cells were resistant to begin with and then outgrow their competitors.
 This is one of the reasons why targeting mutations in driver genes is a
 good start, but needs to be augmented with knowledge of the dynamic changes
 it induces in the cellular signaling network.
\end_layout

\begin_layout Standard
Let us revisit one of the most well-known examples in targeted therapies,
 the inhibitor Plexicon (PLX4720; later marketed with the name Vemurafenib)
 specifically targeted to 
\begin_inset Formula $BRAF^{V600E}$
\end_inset

 mutants.
 In a seminal paper, 
\begin_inset CommandInset citation
LatexCommand cite
key "Chapman2011-jl"

\end_inset

 showed that this example of targeted therapy works with the intended effect
 in such that it kills cancer cells to an extent that makes multiple from
 the outside clearly visible tumours completely disappear for months, which
 could be hailed as a success for identifying a target and rationally designing
 an inhibitor that abrogates oncogenic signaling.
 However, there was one drawback: after a couple more weeks after the initial
 success, the tumour cells were able to overcome the effects of the inhibitor,
 start to grow again, and ultimately lead to the recurrence of the tumours
 that the patient later died from.
\end_layout

\begin_layout Standard
This, together with multiple other examples, proved to show that targeted
 therapies work for a while, but they are, in a lot of cases, unable to
 prevent recurrence.
 But how does this work? In order for a tumour to regrow, there need to
 be some cancerous cells left alive after treatment, as this rapid regrowth
 of mass can not be explained by an independent inception, so some cells
 must have survived the process of therapy.
 More specifically, there are two possibilities of these cells that later
 repopulate a tumour can be formed (and a spectrum in between): (1) the
 applied selection pressure (the therapy killing off sensitive cells) caused,
 together with a rapid accumulation new mutations, different pools or cells
 to emerge where a subset was no longer affected by the drug and that could
 later expand, or (2) those resistant cells were there all along, just expanding
 their niche once the predominant clones were gone.
\end_layout

\begin_layout Standard
This debate lasted for a while, evidence presented for both one [*] and
 the other [*]: surely, if there are cells present that harbour a mutation
 that confers resistance to a given drug at the beginning of treatment,
 sequencing the tumour must show a minor frequency of this mutation that
 later takes over already at that stage.
 However, there are a couple of challenges when trying to assess the whole
 genetic diversity of a sample.
 For instance, tumours are often histologically quite different already
 across different section, implying that the molecular makeup is not uniformly
 distributed.
 Thus, it is important to sample not only in one place, but either mush
 the whole thing up or selectively take small samples in different subsections.
 Another challenge is that for detecting very minor frequencies, one needs
 to sequence at a high depth and be sure that the resulting variants are
 not only due to sequencing errors (if there is little enough DNA that it
 needs to be amplified, errors introduced by PCR need also be considered).
 It eventually settled when some groups [*] were able to overcome those
 challenges and show that the resistant clone is indeed present at low frequency
 at the beginning of therapy.
\end_layout

\begin_layout Subsection
Overcoming resistance
\end_layout

\begin_layout Standard
evol bottlenecks: ***
\end_layout

\begin_layout Standard
meki+brafi combo: 
\begin_inset CommandInset citation
LatexCommand cite
key "Long2014-uy"

\end_inset


\end_layout

\begin_layout Section
Disease models
\end_layout

\begin_layout Subsection
Rationale
\end_layout

\begin_layout Standard
It is easy to argue that in order to find better cancer treatments, we first
 need to understand better the molecular mechanisms that drive it.
 This is especially true because the mechanisms involved from the inception
 of cancer up the its progression and spreading are in fact molecular mechanisms
 that have spun out of the normal biological control that cells of an organism
 exert on each other.
 In order to improve our understanding, it is required to collect data about
 the different types, stages, treatments, and similar.
\end_layout

\begin_layout Standard
This poses a problem: it is not possible to perform all of the assays required
 on the actual patients.
 There is just no justification for a patient to undergo surgery if we want
 to know if protein A interacts with protein B.
 Also with treatments, we can not try new treatments not having a strong
 indication that this might be the best known possibility of curing a patient,
 as this would be highly unethical.
 It follows that there is a requirement for some biological system that
 mirrors the disease while, at the same time, is easy to handle in the laborator
y.
 Therein, the points mentioned involve a trade-off: a system that mirrors
 the disease perfectly and is easy to handle does not exist, but there are
 multiple systems (outlined in the sections below) that are closer to one
 or the other.
 Which one to use must be decided in each experiment individually - considering
 its goals, effort, conclusiveness and applicability to the question studied.
\end_layout

\begin_layout Standard
However, if there is a strong indication given those model systems that
 a certain treatment approach is truly beneficial to a certain patient or
 patient group, this needs to be thoroughly tested to make sure that those
 indications previously shown using a model system - that are known to mirror
 a lot of aspects of the actual disease, but never all - actually hold in
 humans as well.
\end_layout

\begin_layout Standard
All drug screening experiments and analysis based thereon in this work,
 while there are potentially many drawbacks associated with it, have been
 carried out on data that was derived from cell lines.
 This can easily explained by the fact that the amount that needed to be
 generated in order to make general statements about perturbed gene expression
 and especially drug response is not feasible with any other model system.
\end_layout

\begin_layout Standard
For cancer-specific gene expression, I used either used the same cell lines,
 or patient data directly from a consortium whose aim it is to molecularly
 characterise a multitude of cancers (The Cancer Genome Atlas consortium,
 TCGA).
 It is also the latter where I obtained survival data from, as more thoroughly
 described in section *.
\end_layout

\begin_layout Subsection
Cell Lines
\end_layout

\begin_layout Standard
One of the easiest model systems that provide a reasonable accurate mirror
 of a lot of the biology involved in the disease are cell lines, which is
 basically taking a bunch of cells from a tumour, and growing them in a
 petri dish for multiple generations (passages).
 An obvious advantage of this method is that the biological material that
 assays can be performed on is virtually unlimited, easy to produce and
 maintain (the cells grow in the dishes as long as they are supplied with
 nutrients and potentially growth factors or cytokines), while still reflecting
 a lot of the properties that cells in a tumour would.
 However, by passaging cells consecutively in petri dishes for many generations,
 the evolutionary pressure that they are selected by is essentially the
 growth rate on the dish, which will not reflect the proportions that one
 would find in a real tumour.
 In fact, the fastest growing clone will outcompete all others in a couple
 of generations - which gives a relatively uniform molecular phenotype across
 all cells, but in turn also loses the heterogeneity found in a primary
 sample.
\end_layout

\begin_layout Standard
Today, there is a multitude of cell lines available from commercial suppliers.
 The maybe best-known example is that of the HeLa cell line 
\begin_inset CommandInset citation
LatexCommand cite
key "Gey1952-qc"

\end_inset

 that was derived from a patient of cervical cancer in 1951, named Henrietta
 Lacks.
 This cell line, however, with how long it has been kept in culture as well
 as its high mutation rate have produced a genotype that no longer resembles
 primary cancer samples in many ways.
 One property is that a normal human cell has two copies of the genome (the
 two alleles), whereas HeLa cells have four copies, but the HeLa genome
 revealed more extensive aberrations 
\begin_inset CommandInset citation
LatexCommand cite
key "Landry2013-sk"

\end_inset

.
\end_layout

\begin_layout Standard
In contrast to this cell line, most on which experiments are performed on
 more closely resemble their origin: they have, albeit with aberrations,
 a genome that somewhat resembles primary tumours; they also in many cases
 still resemble their tissue of origin in terms of gene expression patterns
 and transcription factor activities.
 However, there are biological dynamics involved in a real tumour that can
 not be recapitulated by cell lines.
 These include interactions with other cell types (where a co-culture is
 possible, but not anywhere close in complexity), interactions with the
 extracellular matrix (remember the flat plastic dishes they are grown in).
 Another one, which can be seen as a special case of the former, is the
 lacking interaction with immune cells.
\end_layout

\begin_layout Subsection
Organoids
\end_layout

\begin_layout Standard
While the overall number of available cell lines is vast, there are limits
 to what is available.
 With the approximately 1,000 cell lines that the Genomics of Drug Sensitivity
 in Cancer (GDSC, cf.
 section **) screened for compound sensitivity 
\begin_inset CommandInset citation
LatexCommand cite
key "Iorio2016-gh"

\end_inset

, it is time to think about how many more cell lines would be needed to
 gain enough statistical power for the long tail of rare mutations and whether
 the required amount of diversity can theoretically be generated with cell
 lines 
\begin_inset CommandInset citation
LatexCommand cite
key "Francies2015-fs"

\end_inset

.
 While every xenografted mouse model is different, it is very time consuming
 and expensive to investigate cancers this way.
 Hence, there is a need for a model system that is simpler and easier to
 handle than animal models, but provides more diversity and numbers than
 cell lines 
\begin_inset CommandInset citation
LatexCommand cite
key "Kamb2005-zq"

\end_inset

.
\end_layout

\begin_layout Standard
One answer that resolves this issue could be organoid cultures: these are
 small assemblies of primary cells taken out of a tumour and grown in matrigel,
 where the spatial separation of mini-cultures retains much of the tumour
 heterogeneity 
\begin_inset CommandInset citation
LatexCommand cite
key "Sachs2014-dz"

\end_inset

.
 Because of these properties, they could advance 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
latin{in silico}
\end_layout

\end_inset

 cancer screening in a way that is not possible with other platforms 
\begin_inset CommandInset citation
LatexCommand cite
key "Francies2015-fs"

\end_inset

.
\end_layout

\begin_layout Subsection
Animal models
\end_layout

\begin_layout Standard
Of course, there are aspects missing when relying on models that handle
 individual cells or the interaction of a couple of cell of the same type
 when trying to assess either mechanisms of pathogenesis or in vivo response
 to a compound, which makes it necessary to perform experiments in a whole
 organism.
 For this purpose animals have long been used, down from a simple multicellular
 worm (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
species{Caenorhabdilis elegans}
\end_layout

\end_inset

, mostly for development and its simple central nervous system) to frogs
 (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
species{Xenopus}
\end_layout

\end_inset

, for induced pluripotency), up to mammals (mice, rats, and chimps).
 Just as the complexity of these organisms increases, they also mirror more
 and more aspects of human physiology.
 However, experiments performed in the latter organisms also takes much
 more time, they are more difficult (and expensive) to set up, and there
 are ethical concerns on keeping, handling, and killing animals for the
 purpose of ultimately saving human lives.
 Still, none of the animals mentioned is human, so there the unknown factors
 as to whether a certain drug that has been shown to work in cell lines
 and mice does indeed have the same effect in humans remains.
 Looking at the number of articles published about a specific molecular
 mechanism or the detailed characterisation of a compound and its usability
 as a drug, mice seem to be considered a reasonable trade-off between providing
 a close enough match of human biology and being not too difficult to keep.
\end_layout

\begin_layout Standard
One line of experiments that can be done with mice but not humans is to
 edit genes in their germline, and then breed them with a reporter or until
 their offspring is homozygous for a desired mutation.
 The gene introduced can either be a reporter, to see where a it is expressed,
 or for instance a mutated version of a human gene that is known to induce
 the formation of tumors.
\end_layout

\begin_layout Standard
Another method of investigation are xenografts, where an immunodeficient
 [*?*] mouse is inoculated with cancerous cells derived from either a cell
 line or a patient, to study effects like how cancerous cells influence
 and modify the microenvironment around them, how they can sustain themselves
 and grow, attract the formation of blood vessels, and ultimately metastasise.
\end_layout

\begin_layout Section
Molecular data types and assays
\end_layout

\begin_layout Subsection
Role of molecular data
\end_layout

\begin_layout Standard
One of the issues of both understanding as well as preventing or treating
 cancer development and progression is that it is a process in such complexity
 that a simple observation of patients with the disease will not suffice
 to come up with effective models of disease inception and progression,
 or treatments for that matter.
 Fortunately, we have got a battery of tests available, quantifying different
 molecular aspects of a biological sample.
 Examples of these data include the sequence, structure, and modification
 of DNA, but also the expression of RNA or proteins, including modifications.
 These different layers, able to quantify different projections of a cell's
 state, have provided us with an unprecedented opportunity to truly understand
 many molecular mechanisms that govern the processes running in both a normal
 and a diseased cell, and the differences between them.
\end_layout

\begin_layout Subsection
DNA
\end_layout

\begin_layout Itemize
sanger seq of few genes
\end_layout

\begin_layout Itemize
NGS
\end_layout

\begin_layout Itemize
methylation
\end_layout

\begin_layout Subsection
RNA
\begin_inset CommandInset label
LatexCommand label
name "sub:assay_RNA"

\end_inset


\end_layout

\begin_layout Standard
In contrast to DNA, which is stable in sequence, and its chromatin structure
 and methylation that are still, albeit a bit less stable, the RNA that
 is transcribed in order to perform some regulatory functions as well as
 produce the proteins that will carry out the bulk, represents more the
 state a cell is currently in and functions that need to be performed at
 a given time and in a given context.
 While there are housekeeping genes whose expression will not change fundamental
ly between different tissue types and situations that a cell is in, the
 specific RNAs whose proteins will mediate specific functions such as cell
 division or response to a stimulus will.
 It can thus be argued that the sum of RNAs (the transcriptome), or more
 specifically mRNAs provides an overview of the state a cell is currently
 in.
 Hence, measuring those RNAs will allow us to get a picture of the processes
 going on in a cell at a given time.
 The two ways by which this is usually done are either microarrays or RNA
 sequencing, as outlined below.
\end_layout

\begin_layout Subsubsection
Microarrays
\end_layout

\begin_layout Standard
The classic way to measure RNA expression is using microarrays 
\begin_inset CommandInset citation
LatexCommand cite
key "Brown1999-mc,Debouck1999-ec"

\end_inset

.
 These are, in the simplest case, a carrier matrix (glass can be used but
 the more recent chips all feature a polymer matrix) that has fixed single-stran
ded oligonucleotides spotted on them.
 For our sample, we would then extract and clean the RNA while simultaneously
 degrading any DNA that it contains, and label the RNA either with one dye
 
\begin_inset CommandInset citation
LatexCommand cite
key "Gohlmann2009-ac,Du2008-sb"

\end_inset

 (if we want to quantify the expression levels of transcripts in a given
 sample) or two different dyes 
\begin_inset CommandInset citation
LatexCommand cite
key "Shalon1996-pq"

\end_inset

 (if we want to quantify the difference between two samples).
 Such fluorescent dyes used are e.g.
 Cy3 and Cy5, that emit light in the green and red wave lengths upon stimulation
, respectively.
 Note that if the technical reproducibility of the platform is good enough,
 it is also possible to compare conditions by using two different arrays
 where we measure each sample individually and then compare the outcome.
 - A technique that has been favoured by companies like Affymetrix as it
 allowed chips at much higher density and the comparison between each sample
 in a control- and in a experimental condition, as well as between them.
\end_layout

\begin_layout Standard
An obvious drawback of this technology is that we need to know in advance
 which oligonucleotides to spot, so in turn which genes we are looking for.
 There needs to be different chips for different organisms, depending on
 the gene sequences that they carry.
 This selection of genes that we look at can of course introduce bias, in
 such that we can't look for transcripts that we don't know exist, but also
 we might focus on transcripts and isoforms that we think are important
 before carrying out an experiment.
 Another possible issue is their detection threshold.
 As the readout is fluorescence based and there will always be a base-line
 level of it, it is hard to quantify the amount of RNA bound on a spot of
 the total number of molecules are so low that a few molecules do not significan
tly change the readout.
 Turning this argument around, RNA binding to spots also has its point of
 saturation, where it does not matter if there is more RNA present or not
 once all probes bind to their complementary sequence and thus produce a
 signal.
\end_layout

\begin_layout Subsubsection
RNA-seq
\end_layout

\begin_layout Standard
An alternative approach 
\begin_inset CommandInset citation
LatexCommand cite
key "Marioni2008-lb,Wang2009-ge"

\end_inset

 that has become more popular recently with the falling costs of DNA sequencing
 is to apply this technology to RNA as well.
 In this case, we can use the RNA that we isolated from a sample, reverse-transc
ribe it into DNA, and sequence this DNA like we would normally.
 This has got the advantage that it can be used even if there is no genome
 sequence or chip available where as a consequence we are also not biasing
 or selection of genes by previously knowing what to look at 
\begin_inset CommandInset citation
LatexCommand cite
key "Trapnell2010-le"

\end_inset

, with the exception of sections in the RNA that can be more or easily transcrib
ed into DNA (like GC-rich regions that also pose challenges for sequencing).
 Another advantage is that the dynamic range this technology is able to
 quantify transcripts is much higher when looking at the reads it produces
 than it is for microarrays, both on the lower as well as on the upper bound:
 we can detect a single read, but also having a lot of reads won't saturate
 our reads the same way that spots on the microarray do (although if a large
 proportion of the reads is from a highly abundant molecule, this will decrease
 sensitivity for detecting other RNAs).
\end_layout

\begin_layout Standard
The way RNA-seq quantification 
\begin_inset CommandInset citation
LatexCommand cite
key "Mortazavi2008-lh,Li2011-zi"

\end_inset

 is usually done is to align the reads to a known genome, allowing gaps
 in the alignment for splicing and differential use of exons 
\begin_inset CommandInset citation
LatexCommand cite
key "Trapnell2012-ds"

\end_inset

.
 The transcription level of a gene, transcript or exon are then quantified
 by how many reads align to the corresponding section in the genome.
 As this is computationally very expensive, there have been efforts recently
 to get around performing the alignment step by k-mer counting and so-called
 pseudo-alignments.
 Methods for the latter include Sailfish 
\begin_inset CommandInset citation
LatexCommand cite
key "Patro2014-wf"

\end_inset

, Salmon 
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/COMBINE-lab/salmon"

\end_inset


\end_layout

\end_inset

, or Kallisto 
\begin_inset CommandInset citation
LatexCommand cite
key "Bray2016-gx"

\end_inset

.
\end_layout

\begin_layout Standard
The data produced by RNA-sequencing is different to the one from microarrays,
 as it is based on read numbers that thus counts (so discrete data) instead
 of the continuous fluorescence signal that microarrays provide.
 These counts are well-known to be negatively binomially distributed, and
 require specialised software packages to call e.g.
 differential expression (the most well-known of which are 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{edgeR}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{DEseq}
\end_layout

\end_inset

, and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{DEseq2}
\end_layout

\end_inset

) or transformation before they can be used in standard linear modelling
 techniques (for instance provided by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
code{voom}
\end_layout

\end_inset

 in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{limma}
\end_layout

\end_inset

 package).
\end_layout

\begin_layout Subsection
Proteins and phosphorylation
\end_layout

\begin_layout Standard
It can easily be argued that in order to get an appropriate picture of what
 goes on in a cell it would be better to look at the proteins that perform
 most functions, as opposed to the mRNA levels of genes.
 The latter will in many cases be translated into proteins that then exert
 their activity, yet mRNA measurements are one more step removed from the
 functional process than proteins, and measuring post-translational modification
s (PTMs) that modify activity already gives us a lot of functional information
 - as long as we know how to interpret them.
\end_layout

\begin_layout Standard
Taking these facts together, one might want to look at proteins instead
 of gene expression.
 Yet, gene expression is a lot cheaper and easier to measure, as well as
 providing more coverage than the proteomic methods currently can.
 Also, the publicly available gene expression data far exceeds the one of
 proteomic data.
\end_layout

\begin_layout Subsubsection
Reverse-Phase Protein Arrays (RPPA)
\end_layout

\begin_layout Standard
In this method 
\begin_inset CommandInset citation
LatexCommand cite
key "Tibes2006-fg"

\end_inset

, there is an array of spotted antibodies, similar to the nucleic acids
 on a microarray.
 A labelled cell lysate (or other sample) is incubated with the antibodies,
 and we can quantify the amount of the proteins (or phospho-proteins) correspond
ing to each antibody afterwards.
 In contrast to microarrays, however, this is relatively low throughput
 as antibodies can not be as readily synthesized as nucleic acids.
\end_layout

\begin_layout Subsubsection
Mass spectrometry
\end_layout

\begin_layout Standard
Mass spectrometry 
\begin_inset CommandInset citation
LatexCommand cite
key "Domon2006-au,Bensimon2012-iy"

\end_inset

 is based on fragmenting proteins into peptides that are then ionized and
 sprayed into an electrical field in vacuum tube, where their mass-to-charge
 ratio causes them to behave in a certain way.
 For instance, in Time of Flight (TOF) instruments, the electrical field
 is used to accelerate the peptides; the force that accelerates them is
 then proportional to their charge, and their inertia to their mass - hence
 the time it takes to reaches the detector is proportional to the ratio
 of the two.
\end_layout

\begin_layout Standard
For more complex samples, there are too many peptides to detect at any given
 time, so the Mass Spectrometry is often coupled with another technology
 that first separates proteins contained in a sample using some other property
 (e.g.
 their hydrophilicity/hydrophobicity) in columns of gas (gas chromatography,
 GC) or liquid (high performance liquid chromatography, HPLC).
\end_layout

\begin_layout Section
Data sets
\end_layout

\begin_layout Subsection
Public gene expression repositories
\end_layout

\begin_layout Standard
There are two major repositories of gene expression experiments using microarray
s, the Gene Expression Omnibus (GEO) 
\begin_inset CommandInset citation
LatexCommand cite
key "Barrett2007-dy,Barrett2009-nh"

\end_inset

 at the National Institutes of Biotechnology Information (NCBI) and ArrayExpress
 
\begin_inset CommandInset citation
LatexCommand cite
key "Parkinson2007-bf,Parkinson2009-ej"

\end_inset

 at the European Bioinformatics institute.
 With the reporting standards defined in the Minimal Information about a
 Microarray Experiment (MIAME) 
\begin_inset CommandInset citation
LatexCommand cite
key "Brazma2001-ro"

\end_inset

, they played a pivotal role to ensure that experiments where one could
 not only measure one gene but the entire transcriptome remained interpretable.
 Both of these are synchronised, which means that if you submit to one the
 other will import the data set and make it available on their platform
 as well.
 More recently, ArrayExpress also started collecting RNA-seq and ChIP-seq
 experiments.
\end_layout

\begin_layout Subsection
Gene sets and pathways
\end_layout

\begin_layout Standard
With about 22,000 human genes whose transcription can be all measured simultaneo
usly using the methods described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:assay_RNA"

\end_inset

, it is important to put genes together into functional groups for several
 reasons: (1) groups of higher level processes are more interpretable, (2)
 gene level measurements in microarrays (and to a lesser extent RNA-sequencing)
 are inherently noisy and the more measurements we combine the clearer the
 signal gets, and (3) it may be possible to solve statistical problems where
 the number of observations per sample is greater than the number of samples.
\end_layout

\begin_layout Standard
There are multiple databases available that link gene sets into function
 groups.
 The most well known are Gene Ontology and the pathway resources KEGG and
 Reactome, as listed below.
\end_layout

\begin_layout Subsubsection
Gene Ontology
\end_layout

\begin_layout Standard
Gene Ontology (GO) 
\begin_inset CommandInset citation
LatexCommand cite
key "Ashburner2000-xl,Gene_Ontology_Consortium2004-sn"

\end_inset

 is 
\begin_inset Quotes eld
\end_inset

a major bioinformatics initiative to develop a computational representation
 of our evolving knowledge of how genes encode biological functions at the
 molecular, cellular and tissue system levels
\begin_inset Quotes erd
\end_inset

.
 It has encoded over 40,000 biological concepts based on experiments reported
 in over 100,000 publications according to its web portal at h
\begin_inset CommandInset href
LatexCommand href
target "ttp://geneontology.org/"

\end_inset

.
\end_layout

\begin_layout Subsubsection
KEGG
\end_layout

\begin_layout Standard
KEGG 
\begin_inset CommandInset citation
LatexCommand cite
key "Kanehisa2000-mp"

\end_inset

 was one of the first pathway databases.
 To provide an idea of how widely it was (and still is) used, Google Scholar
 (query 25th February 2016) reported 6863 publications that cited the original
 article.
 However, in 2XXX the platform went commercial, allowing access only via
 a subscription-based portal.
 While the authors removed public access also to earlier versions of the
 database, its pre-commercial license (OICR by Pathway Solutions, Inc) allowed
 it to still be used.
 The public version, however, will not receive any more updates.
\end_layout

\begin_layout Subsubsection
Reactome
\end_layout

\begin_layout Standard
Reactome 
\begin_inset CommandInset citation
LatexCommand cite
key "Croft2011-po"

\end_inset

 is a 
\begin_inset Quotes eld
\end_inset

a free, open-source, curated and peer reviewed pathway database
\begin_inset Quotes erd
\end_inset

 hosted and curated by the European Bioinformatics Institute (EBI).
 It has a web portal at 
\begin_inset CommandInset href
LatexCommand href
target "http://www.reactome.org/"

\end_inset

, and the current version is v55 released in December, 2015.
 It has an open license (Creative Commons Attribution 4.0
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "https://creativecommons.org/licenses/by/4.0/"

\end_inset


\end_layout

\end_inset

) for its own data and the pre-commercial license for imported KEGG data.
\end_layout

\begin_layout Subsection
Primary cancer cohorts
\end_layout

\begin_layout Subsubsection
The Cancer Genome Atlas (TCGA)
\end_layout

\begin_layout Itemize
inception
\end_layout

\begin_layout Itemize
cohorts
\end_layout

\begin_layout Itemize
data access regulations
\end_layout

\begin_layout Itemize
secondary dbs that simplify access: e.g.
 BROAD GDAC firehose
\end_layout

\begin_layout Subsubsection
International Cancer Genome Consortium (ICGC)
\end_layout

\begin_layout Itemize
umbrella effort
\end_layout

\begin_layout Itemize
more cohorts
\end_layout

\begin_layout Itemize
horrible metadata
\end_layout

\begin_layout Itemize
where data comes from
\end_layout

\begin_layout Itemize
again: unbiased approach = spurious results
\end_layout

\begin_layout Subsection
Cancer cell line drug sensitivity resources
\end_layout

\begin_layout Subsubsection
Cancer Cell Line Encyclopedia (CCLE)
\end_layout

\begin_layout Itemize
status 2002
\end_layout

\begin_layout Subsubsection
Genomics of Drug Sensitivity in Cancer (GDSC)
\end_layout

\begin_layout Itemize
status 2002
\end_layout

\begin_layout Itemize
now: new dataset
\end_layout

\begin_layout Subsection
The Connectivity Map
\end_layout

\begin_layout Subsubsection
Original Connectivity Map using microarrays
\end_layout

\begin_layout Standard
That first large-scale project providing signatures of drug-perturbed gene
 expression changes in the MCF-7 cell line was the Connectivity Map 
\begin_inset CommandInset citation
LatexCommand cite
key "Lamb2006-mp"

\end_inset

.
\end_layout

\begin_layout Subsubsection
The L1000 platform
\end_layout

\begin_layout Standard
The new version of the Connectivity Map is based on Luminex beads 
\begin_inset CommandInset citation
LatexCommand cite
key "Peck2006-fa"

\end_inset

 that are able to measure about 500 transcripts.
 For the L1000 platform
\begin_inset Foot
status open

\begin_layout Plain Layout
More information is available at: 
\begin_inset CommandInset href
LatexCommand href
target "http://www.lincscloud.org/l1000/"

\end_inset


\end_layout

\end_inset

, the BROAD institute managed to put twice the amount of genes on one bead
 by scanning it in two different dilution ranges and deconvoluting them
 computationally afterwards.
 After that, they scaled the experimental readout by 80 control genes that
 are supposed to be constant across experiments, optionally infer the whole
 transcriptome from their 978 
\begin_inset Quotes eld
\end_inset

landmark
\begin_inset Quotes erd
\end_inset

 genes (projections shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:LINCS-projections"

\end_inset

), and compute z-scores (number of standard deviations of a perturbed condition
 over the mean of the control) for each signature.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/3.1_projections.svg
	lyxscale 50
	width 90text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Overview of number of experiments available using cancer drugs.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:LINCS-projections"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In 2015, the BROAD institute released the raw data and z-scores between
 each control- and drug-perturbed experiment for 978 oligonucleotide probes
 and a total of 1.4 million conditions in a 111 gigabyte 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
file{.gctx}
\end_layout

\end_inset

 (HDF5 and metadata) file where they projected their actual measurement
 to the full gene space using publicly available microarray data.
 At this time, no one knew about the quality of the data (the authors claimed
 it to be equal with microarrays; my own tries and conversations with other
 people using the data have indicated that it is below that).
 The main publication is still not out today (June 2016), and they refused
 to share e.g.
 the linear transformation matrix they used to obtain the projected gene
 space.
\end_layout

\begin_layout Standard
Nonetheless, the sheer size of the new dataset will make it very valuable
 for signature matching (cf.
 section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Signature-Matching"

\end_inset

) approaches--more on this in chapter 5.
\end_layout

\begin_layout Section
Computational methods
\end_layout

\begin_layout Standard
Over the last decades and years, biology moved from an observational, qualitativ
e science to a very much quantitative one.
 This can largely be attributed by high throughput assays, led by the advance
 of DNA sequencing and followed by its derivatives as well as other approaches,
 like high-content phenotypic screening - each allowing for measuring not
 one data point, but hundreds and sometimes thousands at a time.
 The magic component in this process is automation: the bulk of the data
 generated by scientific laboratories are not longer carried out by individual
 scientists pipetting together reagents, but by machines controlled by computers
 and designed by both scientists and industry alike.
 This shift of paradigm has not only produced much more data, but arguably
 led to an increase in the quality of such data as well, by removing the
 human element (mood, ability to concentrate, etc.) from large parts of the
 outcome of an experiment.
\end_layout

\begin_layout Standard
This transformation has not only come with its benefits, but also with its
 challenges.
 Setting aside the view of some prominent scientists that each experiment
 needs to have a specific question or hypothesis in mind and that generating
 data first and using the data itself as a hypothesis generator is an inferior
 approach (as best demonstrated by Sidney Brenner's quote 
\begin_inset Quotes eld
\end_inset

low input, high throughput, not output
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Friedberg2008-cn"

\end_inset

), a biomedical scientist's skill requirements have moved from knowing,
 for instance, as many details about a given gene as possible to understanding
 how to set up reproducible assays, generate reliable data, but especially
 treating the resulting data in a way that yielded biological insights.
\end_layout

\begin_layout Standard
There are two crucial parts to this: one is algorithms, that transform the
 raw data generated by a given experiment into biologically interpretable
 indications, and the second is statistics, to make sure the effect observed
 is due to the data and not due to random chance while analysing the results.
 I will outline some very successful ideas that in turn led to algorithms
 and usable software packages for investigating molecular data, both in
 terms of cancer as well as in general, in the sections of this chapter.
\end_layout

\begin_layout Standard
However, first let us reconsider the goal we are trying to achieve in terms
 of this thesis as well as a lot of the related work: we ultimately want
 to improve how cancer is diagnosed and treated for patients, and those
 algorithms help us to define markers of their pathogenesis or how they
 could reap benefits from being offered a particular treatment 
\begin_inset CommandInset citation
LatexCommand cite
key "Rubio-Perez2015-ep"

\end_inset

.
 We thus want to transform the numbers comprising the molecular data we
 have in a meaningful way so they connect to this endeavour.
\end_layout

\begin_layout Subsection
Patterns of mutations
\begin_inset Foot
status open

\begin_layout Plain Layout
Parts of this section have been published with Francesco Iorio 
\begin_inset CommandInset citation
LatexCommand cite
key "Schubert2014-gt"

\end_inset

.
 The state of the text below is derived from an early draft that was written
 entirely by myself.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
When looking at sets of mutations instead of individual ones, the occurrence
 of those patterns has been quantified by a couple of approaches, identifying
 modules that are either co-occurring or mutually exclusive 
\begin_inset CommandInset citation
LatexCommand cite
key "Babur2015-ap"

\end_inset

.
 In the event of co-occurring mutations, this might mean that a given mutation
 is not sufficient to obtain a certain trait and thus a second mutation
 is necessary to confer it.
 In the case of mutual exclusivity, a second mutation is not conferring
 a growth advantage after the first arose.
 This may be due to evolutionary parsimony or a fitness defect.
 In the first case, a second mutation that inactivates, for example, an
 already inactivated tumour suppressor pathway is unlikely to happen on
 the population-level by the pure fact that it is not selected for, as the
 required trait has already been acquired by the cell.
 In the second case, the growth advantage conferred by one mutation might
 be cancelled out or counteracted by the presence of a second one, thereby
 mediating a selection growth disadvantage (fitness defect) of the cells
 carrying both mutations as opposed to either one of them.
 Much focus has been on identifying driver mutations in different cancer
 types, that, combined with oncogene addition (i.e., the tendency of a transformed
 cell to become dependent on sustained impact of the lesion it first obtained)
 forms the basis of many targeted therapies: if you can inhibit signaling
 stemming from a mutation that drives a cancer's development and progression,
 the affected cells' growth will be severely inhibited.
\end_layout

\begin_layout Standard
The concept of a cell being unable to acquire two mutations that together
 have a damaging effect up to the level of cell death has been termed synthetic
 lethality and has been used to study genetic interactions in model organisms
 for a long time 
\begin_inset CommandInset citation
LatexCommand cite
key "Nijman2011-zf"

\end_inset

, where the loss of both genes in a synthetic lethal pair is, as the name
 suggests, lethal to the organism, but the loss of each individual gene
 is not.
 More recently, these interactions have been used to identify si/sh-RNA
 probes that are purified from a starting population 
\begin_inset CommandInset citation
LatexCommand cite
key "Cheung2011-be"

\end_inset

 and efficient computational algorithms have been developed to find those
 pairs in primary cancer data sets like the TCGA 
\begin_inset CommandInset citation
LatexCommand cite
key "Ciriello2012-hc,Gobbi2014-xz"

\end_inset

.
 These interactions can then be transformed in a network of synthetic lethal
 pairs 
\begin_inset CommandInset citation
LatexCommand cite
key "Jerby-Arnon2014-fn"

\end_inset

], which can then be used to predict sensitivity of cancer cell lines to
 a certain drug treatment, as well as clinical outcome given the level of
 co- expression of its pairs.
 Examples of such pairs are a VHL Loss of Function (LoF) mutation that is
 associated with increased sensitivity to not previously associated with
 cancer 
\begin_inset CommandInset citation
LatexCommand cite
key "Bommi-Reddy2008-nc"

\end_inset

, or the well-known BRCA1/2 loss of function and PARP inhibitors 
\begin_inset CommandInset citation
LatexCommand cite
key "Farmer2005-cp"

\end_inset

.
\end_layout

\begin_layout Subsection
Gene expression clustering
\end_layout

\begin_layout Standard
Cancer is known to be a heterogeneous disease, with individual tumours forming
 different subtypes within the same tissue.
 In turn, cells in the same tumour mediate different functions essential
 to its survival and growth.
 One of the arguably most effective ways of elucidating the state of individual
 tumours and cell populations therein is its transcriptome, where transcription
 of individual genes and regulatory RNAs can be assessed using methods such
 as microarrays or RNA sequencing **CMS**.
\end_layout

\begin_layout Standard
Disregarding the effect of different cell types (by making the assumption
 that we sampled a tumour evenly and uniformly while excluding neighbouring
 non-cancerous cells, which is not really the case), we can use the global
 pattern of gene expression to identify different subtypes of the disease
 in a given tissue (or even between tissues).
 A challenge with this is is that the transcriptome could be comprised of
 up to 22,000 genes without even considering most regulatory or ribosomal
 RNAs.
 This space needs to be reduced to something more manageable for visual
 inspection, as well as different subgroups within assigned that have prognostic
 or therapeutic relevance.
 Approaches the allow for visual inspection of a high-dimensional data set
 are called dimensionality reduction techniques, while methods assigning
 the different samples to different subtypes are called clustering algorithms.
\end_layout

\begin_layout Subsubsection
Visualising patterns in high-dimensional data
\end_layout

\begin_layout Standard
Principal Component Analysis (PCA) 
\begin_inset CommandInset citation
LatexCommand cite
key "Wold1987-se"

\end_inset

 is one of the simplest linear transformations that rotates samples in 
\begin_inset Formula $N$
\end_inset

-dimensional space (where 
\begin_inset Formula $N$
\end_inset

 is the number of observations per sample) in a way that its projection
 to 
\begin_inset Formula $M$
\end_inset

-dimensional space (the target dimensions - for interpretability usually
 two or three) maximises the variance contained in 
\begin_inset Formula $M$
\end_inset

.
 The rotation is calculated using the matrix factorisation [add fact here],
 which can be simplified to [add here] and yields a unique solution.
 [params] provide the position of a sample in 
\begin_inset Formula $M$
\end_inset

 space where the axes are principal components, and the latter's projection
 back into N called loadings (which represent how much of each original
 axis is contained within the new axes).
 It is important to note the meaning and interpretability of such a decompositio
n is strongly dependent on the amount of variance captured in the reduced
 space.
 PCA is closely related to Singular Value Decomposition (SVD), which has
 applications in dimensionality reduction as well.
\end_layout

\begin_layout Standard
T-distributed Stochastic Neighbour Embedding (t-SNE) 
\begin_inset CommandInset citation
LatexCommand cite
key "Van_der_Maaten2008-kt"

\end_inset

, instead of relying on the most variable global structures, visualises
 local structures in a given data set: Starting from a point in space (the
 sample), additional samples are distributed in its vicinity depending on
 their distance to the original sample, as well as the other points considered
 (that are a subset of the total number of points).
 Using this method, one can only trust the points neighbouring each other
 up to a certain number for which they were considered, as specified by
 the perplexity parameter.
 This method has been re-branded as viSNE 
\begin_inset CommandInset citation
LatexCommand cite
key "Amir2013-od"

\end_inset

 in their cyt software tool
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://www.c2b2.columbia.edu/danapeerlab/html/cyt-download.html"

\end_inset


\end_layout

\end_inset

, and later extended by the original author to use the Barnes-Hut method
 
\begin_inset CommandInset citation
LatexCommand cite
key "Van_der_Maaten2013-io"

\end_inset

 that is usually used in large N-body simulation in astrophysics.
\end_layout

\begin_layout Subsubsection
Clustering
\end_layout

\begin_layout Standard
This is one of the simplest clustering algorithms is K-means 
\begin_inset CommandInset citation
LatexCommand cite
key "Hartigan1979-pe"

\end_inset

 that requires the number of clusters to be known 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
latin{a priori}
\end_layout

\end_inset

.
 It starts with assigning N cluster centres randomly in a given data set,
 and then assigning all samples that have a smaller distance to a given
 centre than the others to that centre.
 These assignments are iterative, which means that once the cluster centres
 have been determined and the samples assigned, the centres are updated
 to correspond to the centre of the samples each cluster is associated with.
 In turn, which samples are associated with which clusters is also updated
 after each time clusters get new samples assigned, until the process converges
 and further updating steps do not change cluster assignments anymore.
 This method is not used in this work.
\end_layout

\begin_layout Standard
Non-Negative Matrix Factorisation (NMF) 
\begin_inset CommandInset citation
LatexCommand cite
key "Lee2001-ju"

\end_inset

 is a matrix factorisation method that decomposes the matrix 
\begin_inset Formula $V$
\end_inset

 (usually with observations in rows and samples in columns) into the matrices
 
\begin_inset Formula $W$
\end_inset

 (with samples in columns and weights for each cluster in rows) multiplied
 with the matrix 
\begin_inset Formula $H$
\end_inset

 (cluster assignment vectors in columns and samples in rows).
 It does not determine the optimal number of clusters by itself, but can
 be run with different numbers of clusters that is later evaluated using
 the cophenetic coefficient (a measure of goodness of fit of the samples
 to the cluster centres).
\end_layout

\begin_layout Standard
There are also other methods (Spectral Clustering 
\begin_inset CommandInset citation
LatexCommand cite
key "Ng2002-zs"

\end_inset

, Latent Dirichlet Allocation 
\begin_inset CommandInset citation
LatexCommand cite
key "Blei2003-oi"

\end_inset

--a modern, Bayesian method for clustering that is an active research topic)
 but they are not used in this work.
\end_layout

\begin_layout Subsection
Differentially expressed subnetworks
\end_layout

\begin_layout Standard
Instead of or in addition to investigating global differences in the patterns
 of gene expression, we might be interested in how a subset of interacting
 proteins differs between known groups of samples.
 So if, for instance, we know that global gene expression clustering is
 partitioning our samples in two groups, we might be interested in which
 genes may be the mediating effectors in a differential phenotype like drug
 response.
 We could of course just compute differential expression for each gene separatel
y and then look at our ranked list (by p-value and effect size of each gene)
 to determine which are the genes that are most deregulated.
 This, however, may miss a group of interacting genes (or proteins, after
 translation) that all change moderately by themselves, but in combination
 will give a stronger signal than any one gene itself.
\end_layout

\begin_layout Standard
In this case we require an algorithm that summarises differential expression
 within a group of interacting genes (or proteins), while providing us with
 a combined score for a given subnetwork.
 If we are looking gene regulatory networks we would use those interactions
 to map gene expression on.
 If we are looking for likely effector complexes of interacting proteins
 we would ideally use proteomics data, but since [*] have shown that most
 variability in protein expression can be explained by variability in the
 mRNA of the corresponding gene, we could also make a good case for looking
 for the gene expression level of interacting proteins.
\end_layout

\begin_layout Subsubsection
Minimum spanning trees
\end_layout

\begin_layout Standard
The problem of finding a differentially expressed connected subnetwork in
 the network of interacting proteins is similar to the problem of a Minimum
 Spanning Tree, that is, selecting only edges and nodes in a graph that
 maximise a score where each nodes adds to and each edge subtracts from
 it.
 As the exhaustive search for an exact solution to this problem is NP-hard,
 there are several approximate solutions that were developed to solve that
 problem.
 One of those is an algorithm called the Prize Collecting Steiner Tree (PCST),
 which expands and shrinks the tree base on an initial assessment of connected
 scores, and whether or not it adds or subtracts a score (the prize) by
 doing that.
 An example of an implementation that uses a PCST on interacting proteins
 is the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{BioNet}
\end_layout

\end_inset

 R package 
\begin_inset CommandInset citation
LatexCommand cite
key "Beisser2010-wa"

\end_inset

, initially used to analyse deregulated subnetworks in Diffuse Large B Cell
 Lymphoma (DLBC).
\end_layout

\begin_layout Subsection
Gene regulatory networks
\end_layout

\begin_layout Standard
An important aspect of gene expression is that its patterns are not randomly
 but hierarchically organised.
 On perturbations on a cell's surface or its interior a signal is propagated
 from its origin up to proteins that bind to DNA and change their expression
 as a response to the stimulus (more on this in the next section).
 The terminal nodes of this signal transduction are called transcription
 factors, that upon binding on the DNA mediate and direct (either in a promoting
 or in an inhibiting fashion) binding of DNA-dependent RNA polymerase (PolII
 [?]) that forms a complex with available factors in order to start transcriptio
n of a factor's target genes.
 The genes transcribed upon activation of transcription factors may in turn
 be transcription factors themselves that cause increased or decreased transcrip
tion of other genes.
 These interactions between different transcription factors are usually
 referred to as a Transcription Factor Network or Gene Regulatory Network.
\end_layout

\begin_layout Standard
There are multiple ways these can be investigated: by characterising which
 transcription factors bind to which genes (which can be done either experimenta
lly, as outlined in section *, or computationally by looking at which sequence
 of nucleotides a given factor is likely to bind), by finding genes that
 change in a coordinated fashion and hypothesise that those might be regulated
 by the same set of factors, or by making inferences which combination of
 which factors is required for a certain gene to be transcribed.
\end_layout

\begin_layout Subsubsection
Binding motifs
\end_layout

\begin_layout Standard
Some, albeit not all, transcription factors prefer to bind a specific sequence
 of nucleotides that the recognize on the DNA strands.
 Depending on how strong this preference is, it may require a certain amount
 of experiments to find a statistical enrichment in the regions it was found
 to bind in terms of the nucleotide sequences that peaks are found in ChIP-seq.
 This preference for a certain string of nucleotides is called a motif.
 The enrichment in ChIP-seq peaks can be found using tools such as MEME
 
\begin_inset CommandInset citation
LatexCommand cite
key "Bailey1995-hs"

\end_inset

 that will report different weight matrices for nucleotides around the binding
 region (which correspond to how often a given nucleotide is found in a
 given position).
 These motifs in turn can be used to scan the genome for the same or a similar
 sequence using a sliding window approach that may indicate sites a transcriptio
n factor could bind but did not in the original experiment.
 Such can for instance be the case if the chromatin is too densely packed
 in a region (but may not be if the cell was in a different state or from
 a different tissue), or the binding site is occupied by another transcription
 factor blocking the binding of the one we are looking at.
 There are databases that collect and store those motifs from experimental
 data and their known and predicted binding in different cell types, such
 as JASPAR 
\begin_inset CommandInset citation
LatexCommand cite
key "Portales-Casamar2009-nd,Mathelier2013-mo"

\end_inset

, TRANSFAC 
\begin_inset CommandInset citation
LatexCommand cite
key "Matys2003-ea"

\end_inset

, or the Ensembl Regulatory Build 
\begin_inset CommandInset citation
LatexCommand cite
key "Zerbino2016-mx"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Mutual Information for transcription factor networks
\end_layout

\begin_layout Standard
Another question that we might be interested in is which genes share a common
 regulator.
 In the simplest case this is a transcription factor that, upon activation,
 transcribes a set of genes in a coordinated fashion.
 Target genes of this transcription factors may act in that capacity themselves,
 so we come back to the construction of a GRN.
 Turning the argument on its head, we can look genes that are expressed
 in a coordinated fashion across different conditions (which includes e.g.
 different patient samples, as long as we can assume that the transcription
 factor-target relationship is the same) by calculating the mutual information
 of all gene pairs, setting a lower bound to consider, and the pruning the
 edges in the resulting network by postulating that a link between genes
 A and C is indirect if the mutual information of A and B, as well as B
 and C is higher than the one of A and C (a concept called Data Processing
 Inequality).
 On top of ARACNE, the authors also propose a method they call Master Regulator
 Analysis (MRA) 
\begin_inset CommandInset citation
LatexCommand cite
key "Basso2005-ql"

\end_inset

 to identify the regulatory network in human B cells.
\end_layout

\begin_layout Standard
In an extension proposed by the same group is to condition the mutual informatio
n on the expression of a modulator that they call MINDy 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang2009-ui"

\end_inset

.
 The idea here is to look at the upper and lower third of the modulator
 expression, and calculate mutual information for both of theses sets.
 Afterwards, instead of inferring the network, the authors look for the
 strongest changes in mutual information between the two subsets: if that
 is the case, the modulator can be seen as a co-factor required for transcriptio
n factor regulation.
 If a high modulator expression corresponds with increased mutual information
 that will likely be activating, or inactivating if in corresponds to decreased
 mutual information.
\end_layout

\begin_layout Subsection
Pathways and signaling
\end_layout

\begin_layout Standard
The transduction of a signal from a cue either binding to a receptor on
 a cell's surface or intracellularly, is how a cell responds to changes
 in its environment.
 In turn, this signal is relayed through the network of kinases and phosphatases
 (there are other chemical modifications, but those are the best studied)
 until it reaches the terminal nodes that are the transcription factors,
 acting in conjunction with polymerase III and other co-factors to initiate
 changes in gene expression.
 Those genes are in turn transcribed into RNA, and if they are protein-coding
 consequently translated into those.
 The process of transcription and translation is referred to the central
 dogma of molecular biology.
\end_layout

\begin_layout Standard
Cell signaling is deregulated in many diseases, including cancer that we
 focus on here because of the sheer wealth of available data.
 But how to best quantify the signalling activity? The closest proxy we
 have would be to quantify post-translational modifications that are known
 to confer activity.
 In the simplest case this could be a phosphate group on a certain position
 that is known to make a kinase active, that is that it in turn phosphorylates
 its downstream targets.
 But phosphorylation data, and to a lesser extent protein data in general
 is much harder to come by than sequencing data.
 Mass spectrometry and Reverse Phase Protein Arrays just do not produce
 the same clarity that a DNA sequence or RNA level provides, plus is much
 harder to generate because the technology that could to it with the same
 throughput as sequencing technologies just does not exist.
\end_layout

\begin_layout Standard
We can thus argue that computational methods are required to make statements
 about cell signaling, starting from DNA and RNA instead of protein and
 PTM data.
 The simplest approach to this is looking how much mRNA of signalling molecules
 are expressed that comprise a pathway, and designate a high expression
 a high activity and vice versa (using algorithms like Gene Set Enrichment
 Analysis 
\begin_inset CommandInset citation
LatexCommand cite
key "Subramanian2005-pd"

\end_inset

, more in chapter 2).
 This, however, is at odds with the way cell signaling works and is regulated.
 Inferring the protein levels from mRNA levels may indeed be viable [60%
 paper], but two steps removed from the PTMs.
 Methods have been developed to address the issue of taking the expression
 level of a gene set as activity proxy by considering the structure and
 signs of the different pathway molecules, e.g.
 Signaling Pathway Impact Analysis 
\begin_inset CommandInset citation
LatexCommand cite
key "Tarca2008-ey"

\end_inset

 or Pathifier 
\begin_inset CommandInset citation
LatexCommand cite
key "Drier2013-vn"

\end_inset

.
 They, however, do still not distinguish between expression level and activity.
 Another method, PARADIGM 
\begin_inset CommandInset citation
LatexCommand cite
key "Vaske2010-zn"

\end_inset

 could in theory support it, depending the pathway structure supplied for
 inference.
 Yet, the focus has never been to tell apart activity from expression.
 I elaborate on these issues further in chapter 2-4, with a possible way
 to resolve them.
\end_layout

\begin_layout Subsection
Signature Matching
\begin_inset CommandInset label
LatexCommand label
name "sub:Signature-Matching"

\end_inset


\end_layout

\begin_layout Standard
When investigating the effect small molecules (i.e., drugs) have on a system,
 we might not want to look at the mechanism by which this happens at all,
 but instead rely on the changes of gene expression upon treatment with
 it 
\begin_inset CommandInset citation
LatexCommand cite
key "Lamb2007-dg"

\end_inset

.
 Then we could use the downstream genes that are changing as a signature
 of treatment with that drug.
 This has got two applications that have indeed been used, which are the
 following: (1) matching a drug's signature with another drug's signature,
 and if they are a close match but have e.g.
 different indications, suggest that each drug may be used for the other
 indication (where directionality may be limited by additional factors),
 or (2) matching a drug's signature with the inverse gene expression changes
 that a disease exhibits over normal controls, we can suggest that this
 drug may be used to counteract the effects of the disease 
\begin_inset CommandInset citation
LatexCommand cite
key "Iorio2009-ji,Pacini2013-xn"

\end_inset

, i.e.
 be a potential treatment if the disease is indeed caused by the gene expression
 changes that the drug reverses.
 Of course, those matches should rather be seen as hypothesis generators
 than definite indications for repurposing and treating diseases.
 Of course, this kind of approach relies on the availability of signatures
 of the drugs we look at, or of a given drug and disease, respectively.
\end_layout

\begin_layout Section
Reproducibility of results
\end_layout

\begin_layout Subsection
Experimental and computational
\end_layout

\begin_layout Standard
Reproducibility of scientific experiments has recently gotten into the spotlight
 when Amgen published a high-profile study that tried to independently validate
 findings of [40?] findings reported by different groups in the journals
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
journal{Nature}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
journal{Science}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
journal{Cell}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker2016-ng"

\end_inset

.
 For half of the studies that they investigated, they could not obtain the
 same results and thus conclusions that the respective authors reported.
 Needless to say, this raised some concerns about the current paradigm of
 publishing new results quickly instead of thoroughly and overclaiming the
 effect or significance of a study in order to have an article accepted
 in the top tier journals.
 Follow-up studies have pointed a part of the blame on use of different
 antibodies 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker2015-vi"

\end_inset

 that did not always show the affinity and specificity to their target as
 the vendors claimed, which has resulted in the creation of a registry for
 validated antibodies at 
\begin_inset CommandInset citation
LatexCommand cite
key "Bradbury2015-is"

\end_inset

.
\end_layout

\begin_layout Standard
It is easy to argue that experimental reproducibility is a hard issue to
 tackle, as the potential for confounding variables is huge and a lab will
 hardly ever have the resources to account for all of them.
 In general, good study design, that is proper controls and randomisation,
 should go a long way (especially in the case of clinical trials), but even
 then, there may be confounding effects unknown to the experimenters and
 not properly controlled for, as has been shown in the case where male vs.
 female stewards in a mouse facility produced different reactions of the
 animals, potentially influencing the results obtained from a very large
 number of studies.
\end_layout

\begin_layout Standard
Apart from experimental reproducibility, computational reproducibility should
 be an issue that is easier to tackle, because at least for deterministic
 algorithms the same input should always produce the same output using the
 same tools.
 However, this is also easier said than done because the tools will depend
 on other tools, maybe with different versions, and each version may change
 the way they treat the data to go from input to output in a slightly different
 way.
 This is especially true with computational analyses, as well as the tools
 employed, are getting more and more complex.
 However, even if the potential confounding factors are smaller than for
 experimental scientists, computational analysis also has its high-profile
 fraught with issues.
 The most well-known example of this are maybe Anil Potti's cancer gene
 signatures
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
target "http://retractionwatch.com/2015/11/07/its-official-anil-potti-faked-data-say-feds/"

\end_inset


\end_layout

\end_inset

 that Keith Baggerly later spent six months trying to reproduce--and failed,
 but in the process found considerable errors that later caused the original
 article to be retracted 
\begin_inset CommandInset citation
LatexCommand cite
key "Baggerly2009-xc"

\end_inset

.
\end_layout

\begin_layout Standard
To summarise, ensuring that a subsequent study has the possibility to arrive
 at the same conclusions given the same starting point, and in turn build
 on the results in a more confident way, has gotten a big enough issue to
 dedicate an introductory chapter to it.
 Since I did not perform any experiments to produce data myself, the focus
 of this chapter shall be to ensure that the results obtained by transforming
 raw (or in some cases already processed) data into other types of data,
 plots, and ultimately interpretation are reproducible in a sense that a
 person not involved in the projects could arrive at the same conclusion,
 being provided this thesis, the code used, and the technical documentation
 written in conjunction with it.
 This is especially true because the maybe best known retraction caused
 by computational irreproducibility handled a top very similar to the one
 I investigate in this thesis: the effect of signaling pathway signatures
 of different cancers (chapter X) and their significance (chapter Y).
\end_layout

\begin_layout Subsection
Scientific software ecosystem
\end_layout

\begin_layout Standard
With computational analyses getting more complex, it is no longer enough
 to just apply a simple statistical test for a number of observations of
 one condition vs.
 another.
 The wealth of data that has become available needs pre-processing, normalising,
 statistical analysis, and interpretation of results.
 No one scientist can perform all of these tasks completely independently,
 as some of them may detailed knowledge of all the algorithms involved,
 up to an extent that is prohibitory.
 It is thus required to package repeated steps together in a higher-order
 functionality.
\end_layout

\begin_layout Standard
This is where the scientific software ecosystem comes in.
 And, to a larger extent, the ecosystem of general software.
 For instance, I want to obtain, pre-process, and normalise microarray data
 to then compute differentially expressed genes in two conditions.
 The concepts of obtain, preprocess, and normalise are well enough defined
 that I should not have to worry about their exact implementation.
 I just need to know what these concepts mean and there is a software that
 abstracts the low-level implementation to a higher-level function that
 I can just apply.
 And, in addition, I want to script those steps together without needing
 to worry about things like the exact memory allocation in each step.
\end_layout

\begin_layout Standard
Two programming languages, along with the packages they themselves as well
 as their users provide, have been fundamental in providing a tool set that
 allows processing, exploration, and analysis of the amount of data contemporary
 experiments provide.
 These are R 
\begin_inset CommandInset citation
LatexCommand cite
key "Ihaka1996-bm"

\end_inset

 and Python 
\begin_inset CommandInset citation
LatexCommand cite
key "Van_Rossum1995-pl"

\end_inset

, along with packages hosted on CRAN/BioConductor 
\begin_inset CommandInset citation
LatexCommand cite
key "Gentleman2004-oc"

\end_inset

 (including 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{dplyr}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Wickham2014-jy"

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
rpkg{ggplot2}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Wickham2011-qn"

\end_inset

, used extensively in the analyses) and PyPI, respectively.
 Without those tools, performing analysis like I do to obtain the results
 shown later would not be possible.
 Note that there are others, but they do not play a similarly important
 role in contemporary data analysis.
\end_layout

\begin_layout Subsection
Reproducible workflows
\end_layout

\begin_layout Subsubsection
Keeping past versions of scripts
\end_layout

\begin_layout Standard
Requirements for analyses are going to change, and so will the scripts that
 were used to generate them.
 There is nothing that is keeping us from updating and changing them of
 course, but at times it is required to reproduce the behaviour and outcome
 of an analysis after that.
 This is very version control systems (VCS) come in.
 These are software that will keep track of all previous versions of code
 (and potentially other files as well) in case they are ever needed again.
 One of the first to be widely used was CVS 
\begin_inset CommandInset citation
LatexCommand cite
key "Thomas2003-ga"

\end_inset

, later subversion 
\begin_inset CommandInset citation
LatexCommand cite
key "Pilato2008-bp"

\end_inset

 and now mostly git 
\begin_inset CommandInset citation
LatexCommand cite
key "Loeliger2012-sf"

\end_inset

.
 A company that was been widely successful in promoting the use of git both
 in general and academia
\begin_inset Foot
status open

\begin_layout Plain Layout
Github has become the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
latin{de facto}
\end_layout

\end_inset

 standard in academic code sharing.
 The technology for version tracking is state of the art, the functionality
 of their website unparalleled and they offer their services to academics
 for free: 
\begin_inset CommandInset href
LatexCommand href
target "https://education.github.com/"

\end_inset


\end_layout

\end_inset

 in particular is Github 
\begin_inset CommandInset citation
LatexCommand cite
key "Dabbish2012-hw"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Defining workflows with a single entry point
\end_layout

\begin_layout Standard
A challenge that often is underestimated is that even when all of the code
 that was used in order to generate the is provided, it is often not trivial
 know which script generates which part of the analysis, which other scripts,
 analyses, or data it depends on.
 Thus it is not only important to provide the code as it is, but also informatio
n on how to run it, so to not only have all the bits and pieces but to connect
 them to a workflow from the data to the results.
 Some tools have been proposed to manage scientific workflows 
\begin_inset CommandInset citation
LatexCommand cite
key "Warr2012-yd,Wolstencroft2013-eu"

\end_inset

, but they are heavy monolithic pieces of software whose use has never taken
 off.
 A simple and lightweight alternative is GNU's 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
code{make}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Stallman1991-hz,Stallman2004-ud"

\end_inset

, which was originally designed for tracking dependencies in software compilatio
n, but has been proposed to enable reproducible scientific workflows 
\begin_inset CommandInset citation
LatexCommand cite
key "Schwab2000-fk"

\end_inset

.
 It has been extensively used in the analyses because of its single entry
 point (the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
file{Makefile}
\end_layout

\end_inset

) and simple definition of dependency rules.
\end_layout

\begin_layout Subsubsection
Generating reports directly from the analysis
\end_layout

\begin_layout Standard
While the above tools take care of performing and keeping track of analyses,
 the crucial point that is missing for reproducible research is integration
 with reporting.
 A tool to do combine R code with written text and annotation (as opposed
 to technical/API documentation) is knitr 
\begin_inset CommandInset citation
LatexCommand cite
key "Xie2014-sa"

\end_inset

 that can work with either Latex or Markdown.
\end_layout

\begin_layout Section
Motivation and outlook
\end_layout

\begin_layout Standard
Cancer sequencing projects 
\begin_inset CommandInset citation
LatexCommand cite
key "The_Cancer_Genome_Atlas_Research_Network2013-bi,International_Cancer_Genome_Consortium2010-bn"

\end_inset

 are giving us an unprecedented view on the mutational landscape of human
 cancers.
 In addition, this information is augmented by genomic and general molecular
 characterisation of cell lines and their responses to anticancer compounds
 
\begin_inset CommandInset citation
LatexCommand cite
key "Garnett2012-dk,Barretina2012-of,Iorio2016-gh"

\end_inset

, or by selection viable genotypes under a selective constraint 
\begin_inset CommandInset citation
LatexCommand cite
key "Cheung2011-be"

\end_inset

.
 These projects have led us to believe that, while this landscape is vast,
 mutations tend to occur in a limited set of pathways, often exhibiting
 a pattern different from the ones one would expect by random chance.
 As these modules deviate from a randomly expected distributions, it is
 commonly accepted that they represent the pattern of evolutionary adaption
 that enabled a cancer to obtain its hallmarks 
\begin_inset CommandInset citation
LatexCommand cite
key "Hanahan2000-is,Hanahan2011-he"

\end_inset

.
 Mutations that are generally selected for and often functionally characterised
 to be tumorigenic are often referred to as driver mutations, while those
 following the background pattern are likely to occur just by amplified
 replication- and DNA repair defects.
\end_layout

\begin_layout Standard
One of the major challenges is how to use the available molecular data in
 an efficient and reproducible manner.
 While this is relatively straightforward for mutational data (mostly used
 as binary features), it gets much for challenging for higher content readouts
 like gene expression or DNA methylation.
 Specifically, expression-derived biomarkers that were able to separate
 distinct populations in a given conditions have often failed to replicate
 their function once the conditions are altered.
 As DNA methylation is dynamic, it will likely offer the same challenges.
 There is still a tremendous amount of information that should be used to
 decide on the optimal treatment, but most approaches have fallen short
 of that promise.
\end_layout

\begin_layout Standard
One way genes expression has shown promise is using global clustering to
 identify subtypes of the disease, and associating those with different
 drug response or survival outcome.
 We believe that stratifying patients in a reproducible manner should involve
 both the overall clustering of the data as well as more specific, functionally
 relevant readouts.
 An example of those functional readouts is the expression level of pre-defined
 pathway sets - mostly using the GSEA scores - that has been reported in
 numerous publications, but also more sophisticated methods that attempt
 to quantify the signal flow, such as SPIA (Tarca et al.
 2008), PARADIGM (Vaske et al.
 2010), or Pathifier (Drier et al.
 2013).
\end_layout

\begin_layout Standard
These methods, however, stand at odds with the notion that signalling in
 animal - and thus human - cells is tightly post-translationally regulated,
 since they are based on the mRNA expression of the signalling proteins,
 in one way or another.
 To date, it is largely unexplored how much those pathway-expression-based
 methods are able to make statements about the signal transmission in a
 protein network.
 We propose a different strategy: use a large body of publicly available
 perturbation experiments to quantify gene expression responses to a specified
 set of stimuli, and then use those genes to infer the upstream signal required
 to change their expression.
 This is one way by which - only using gene expression data - we can indirectly
 observe activity instead of just mRNA expression.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
